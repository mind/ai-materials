# 神经网络前向传播

## 激活函数的概念

介绍前向传播之前，先介绍一些激活函数的概念。在之前的章节中，我们提到，全联接神经网络就是感知器相互联接和叠加得到的，感知器最后的输出，需要经过一个非线性函数$\phi$。这个非线性函数$\phi$称为激活函数。如果没有这个激活函数，感知器的公式就变成了这样

$$
y =w_1x_1+w_2x_2+\cdots+w_nx_n + b
$$
可以看到，这个公式表达的模型只能处理线性关系的数据，而现实中的数据，并不是所有的数据关系都是线性的。

早期神经网络研究中，常使用Sigmoid函数作为激活函数，其函数图像如下：

![212-dl-basics-01-02](212-dl-basics-01/212-dl-basics-01-02.png)

公式为：了
$$
f(z) = \frac{1}{1+\exp(-z)}.
$$
从公式和图像可以看出，该函数的值域为$(0, 1)$。由于该函数可以将输入转换成对应的百分比，也就是可以直接输出置信度，因此比较常用在神经网络的输出中。

该函数的导数为
$$
\begin{align}
f'(z) &= (\frac{1}{1+\exp(-z)})' \\
       &=- \frac{1}{(1+\exp(-z))^2}(1+\exp(-z))' \\
       &= -\frac{1}{(1+\exp(-z))^2}(\exp(-z))' \\
       &= \frac{\exp(-z)}{(1+\exp(-z))^2} \\
       &= f(z)\frac{1-1+\exp(-z)}{1+\exp(-z)} \\
       &= f(z)\frac{(1+\exp(-z))-1}{1+\exp(-z)} \\
       &=f(z) (1-f(z))
\end{align}
$$
这个结论在后续的公式内容推导中会比较常用。

## 神经网络前向传播

参考前面章节提到过的全联接网络：

![212-dl-basics-01-01](212-dl-basics-01/212-dl-basics-01-01.png)

我们用 $  {n}_l $ 来表示网络的层数，本例中 $ n_l=3$ ，我们将第 $ l$ 层记为 $ L_l$ ，于是 $ L_1$ 是输入层，输出层是 $ L_{n_l}$ 。本例神经网络有参数 $ (W,b) = (W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)})$ ，其中 $ W^{(l)}_{ij}$ （下面的式子中用到）是第 $ l$ 层第 $ j$ 单元与第 $ l+1 $层第 $ i $单元之间的联接参数（其实就是连接线上的权重，注意标号顺序）， $ b^{(l)}_i $是第 $ l+1 $层第 $ i $单元的偏置项。因此在本例中， $ W^{(1)} \in \Re^{3\times 3} $， $ W^{(2)} \in \Re^{1\times 3}$ 。注意，没有其他单元连向偏置单元(即偏置单元没有输入)，因为它们总是输出 $ +1$。同时，我们用 $ s_l$ 表示第 $ l$ 层的节点数（偏置单元不计在内）。


我们用 $ a^{(l)}_i$ 表示第 $ l $层第 $ i $单元的激活值（输出值）。当 $ l=1 $时， $ a^{(1)}_i = x_i $，也就是第 $ i$ 个输入值（输入值的第 $ i $个特征）。对于给定参数集合 $ W,b $，我们的神经网络就可以按照函数 $ h_{W,b}(x) $来计算输出结果。本例神经网络的计算步骤如下：

$$
\begin{align}
a_1^{(2)} &= f(W_{11}^{(1)}x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})  \\
a_2^{(2)} &= f(W_{21}^{(1)}x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + b_2^{(1)})  \\
a_3^{(2)} &= f(W_{31}^{(1)}x_1 + W_{32}^{(1)} x_2 + W_{33}^{(1)} x_3 + b_3^{(1)})  \\
h_{W,b}(x) &= a_1^{(3)} =  f(W_{11}^{(2)}a_1^{(2)} + W_{12}^{(2)} a_2^{(2)} + W_{13}^{(2)} a_3^{(2)} + b_1^{(2)}) 
\end{align}
$$

我们用 $ z^{(l)}_i $表示第 $ l $层第 $ i $单元输入加权和（包括偏置单元），比如， $  z_i^{(2)} = \sum_{j=1}^n W^{(1)}_{ij} x_j + b^{(1)}_i $，则 $ a^{(l)}_i = f(z^{(l)}_i)$ 。


这样我们就可以得到一种更简洁的表示法。这里我们将激活函数 $ f(\cdot)$ 扩展为用向量（分量的形式）来表示，即 $ f([z_1, z_2, z_3]) = [f(z_1), f(z_2), f(z_3)] $，那么，上面的等式可以更简洁地表示为：

$$
\begin{align}
z^{(2)} &= W^{(1)} x + b^{(1)} \\
a^{(2)} &= f(z^{(2)}) \\
z^{(3)} &= W^{(2)} a^{(2)} + b^{(2)} \\
h_{W,b}(x) &= a^{(3)} = f(z^{(3)})
\end{align}
$$
我们将上面的计算步骤叫作**前向传播**。回想一下，之前我们用 $ a^{(1)} = x $表示输入层的激活值，那么给定第 $ l $层的激活值 $ a^{(l)} $后，第 $ l+1 $层的激活值 $ a^{(l+1)} $就可以按照下面步骤计算得到：

$$
\begin{align}
z^{(l+1)} &= W^{(l)} a^{(l)} + b^{(l)}   \\
a^{(l+1)} &= f(z^{(l+1)})
\end{align}
$$
将参数矩阵化，使用矩阵－向量运算方式，我们就可以利用线性代数的优势对神经网络进行快速求解。


目前为止，我们讨论了一种神经网络，我们也可以构建另一种结构的神经网络（这里结构指的是神经元之间的联接模式），也就是包含多个隐藏层的神经网络。最常见的一个例子是 $  n_l $层的神经网络，第 $  1 $层是输入层，第 $  n_l $层是输出层，中间的每个层 $  l $与层 $  l+1 $紧密相联。这种模式下，要计算神经网络的输出结果，我们可以按照之前描述的等式，按部就班，进行前向传播，逐一计算第 $  L_2 $层的所有激活值，然后是第 $  L_3 $层的激活值，以此类推，直到第 $  L_{n_l} $层。这是一个前馈神经网络的例子，因为这种联接图没有闭环或回路。


神经网络也可以有多个输出单元。比如，下面的神经网络有两层隐藏层： $ L_2 $及 $ L_3 $，输出层 $ L_4 $有两个输出单元。

![212-dl-basics-01-03](212-dl-basics-01/212-dl-basics-01-03.png)

要求解这样的神经网络，需要样本集  $ (x^{(i)}, y^{(i)}) ​$，其中 $ y^{(i)} \in \Re^2 ​$。如果你想预测的输出是多个的，那这种神经网络很适用。（比如，在医疗诊断应用中，患者的体征指标就可以作为向量的输入值，而不同的输出值 $ y_i ​$可以表示不同的疾病存在与否。）