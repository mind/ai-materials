

# 优化算法

## 梯度下降法

### 梯度下降法算法详解

#### 1. 梯度下降的直观解释

首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。

从上面的解释可以看出，梯度下降不一定能够找到全局最优解，有可能找到的是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

![202-ml-basics-08](202-ml-basics-08/202-ml-basics-08-01-youhua.png)

#### 2. 梯度下降的相关概念

在详细了解梯度下降的算法之前，我们先看看相关的一些概念。

1. 步长(Learning rate)：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。

2. 特征(feature)：指的是样本中输入部分，比如单特征的2个样本$(x(0),y(0)),(x(1),y(1))$，第一个样本特征为$x(0)$，第一个样本输出为$y(0)$。

3. 假设函数(hypothesis function)：在监督学习中，为了拟合输入样本而使用的假设函数，记为$h_\theta(x)$。比如对于单特征的$m$个样本$(x(i),y(i))(i=1,2,...m)$，可以采用拟合函数如下：$h_\theta(x)=\theta_0+θ_1x$。

4. 损失函数(loss function)：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于$m$个样本$(x_i,y_i)(i=1,2,...m)$，采用线性回归，损失函数为：
   $$
   J(\theta_0,\theta_1)=\sum_{i=1}^m(h_\theta(x_i)−y_i)^2
   $$
   其中$x_i$表示第$i$个样本特征，$y_i$表示第$i$个样本对应的输出，$h_\theta(x_i)$为假设函数。

#### 3. 梯度下降的详细算法

##### 3.1 梯度下降法的代数方式描述

梯度下降法算法有代数法和矩阵法(也称向量法)两种表示，代数法比较容易理解，而矩阵法会显得更加简洁，由矩阵实现的逻辑更加一目了然。这里先介绍代数法，后介绍矩阵法。

1. 先决条件： 确认优化模型的假设函数和损失函数。
   比如对于线性回归，假设函数表示为$h_\theta(x_1,x_2,...x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n$，其中$\theta_i(i = 0,1,2,..., n)$为模型参数，$x_i (i = 0,1,2... n)$为每个样本的$n$个特征值。这个表示可以简化，我们增加一个特征$x_0=1$，这样$h_\theta(x_0,x_1,...,x_n)=\sum_{i=0}^nθ_ix_i$。
   同样是线性回归，对应于上面的假设函数，损失函数为：
   $$
   J(θ_0,θ_1,...,θ_n)=\frac{1}{m}\sum_{j=1}^m(h_\theta(x_0(j),x_1(j),...x_n(j))−y_j)^2
   $$

2. 算法相关参数初始化：主要是初始化$\theta_0,\theta_1,...,\theta_n$，算法终止距离$\varepsilon$以及步长$\alpha$。在没有任何先验知识的时候，我喜欢将所有的$\theta$初始化为0， 将步长初始化为1。在调优的时候再优化。

3. 算法过程：
   1)确定当前位置的损失函数的梯度，对于$\theta_i$，其梯度表达式如下：
   $$
   \frac{\partial J(θ_0,θ_1...,θ_n)}{\partial \theta_i}
   $$
   2)用步长乘以损失函数的梯度，得到当前位置下降的距离，即$\alpha\frac{\partial J(θ_0,θ_1...,θ_n)}{\partial θ_i}$。对应前面登山例子中的某一步。

   3)确定是否所有$\theta_i$的梯度下降距离都小于$\varepsilon$，如果小于$\varepsilon$则算法终止，当前所有的$\theta_i(i=0,1,...n)$即为最终参数结果。否则进入步骤4.

   4)更新所有的$\theta$，对于$\theta_i$，其更新表达式如下。更新完毕后继续转入步骤1.
   $$
   θ_i=θ_i−\alpha\frac{\partial J(\theta_0,\theta_1,...,\theta_n)}{ \partial \theta_i}
   $$






下面用线性回归的例子来具体描述梯度下降。假设我们的样本是
$$
(x_1(0),x_2(0),...,x_n(0),y_0),(x_1(1),x_2(1),...,x_n(1),y_1),...,(x_1(m),x_2(m),...x_n(m),y_m)
$$
损失函数如前面先决条件所述：
$$
J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{m}\sum_{j=1}^{m}(h_\theta(x_0(j),x_1(j),...x_n(j))−y_j)^2
$$
算法过程步骤1中对于$θ_i$的偏导数计算如下： 　　
$$
\frac {\partial J(\theta_0,\theta_1,...,\theta_n)}{\partial \theta_i}=\frac{2}{m}\sum_{j=1}^m(h_\theta(x_0(j),x_1(j),...,x_n(j))−y_j)\frac{\partial{h_\theta}}{\partial \theta_i}
$$
由于样本中没有$x_0$，令上式中所有的$x_0(j)$为1。步骤4中$\theta_i$的更新表达式如下：
$$
\theta_i=\theta_i−\frac{2\alpha}{m}\sum_{j=1}^m(h_\theta(x_0(j),x_1(j),...,x_n(j))−y_j)\frac{\partial{h_\theta}}{\partial \theta_i}
$$
从这个例子可以看出当前点的梯度方向是由所有的样本决定的，加$\frac{2}{m}$是为了好理解。由于步长为常数，他们的乘积也为常数，所以这里$\frac{2\alpha}{m}$可以用一个常数$\alpha$表示。

在下面第4节会详细讲到梯度下降法的变种，他们主要的区别就是对样本的采用方法不同。这里我们采用的是所有样本。

##### 3.2 梯度下降法的矩阵方式描述

这一部分主要讲解梯度下降法的矩阵表示方法，相对于2.3.3.1的代数法，要求有一定的矩阵分析基础知识，尤其是矩阵求导的知识。

1. 先决条件： 和2.3.3.1类似， 需要确认优化模型的假设函数和损失函数。对于线性回归，假设函数为
   $$
   h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n
   $$
   分别代入特征值$x_i(1),x_i(2),...,x_i(m)$，$i=0,1,...,n$，得到
   $$
   h_\theta(x_1(1),x_2(1),...,x_n(1))=\theta_0+\theta_1x_1(1)+...+\theta_nx_n(1)
   $$

   $$
   h_\theta(x_1(2),x_2(2),...,x_n(2))=\theta_0+\theta_1x_1(2)+...+\theta_nx_n(2)
   $$

   $$
   ...
   $$

   $$
   h_\theta(x_1(m),x_2(m),...,x_n(m))=\theta_0+\theta_1x_1(m)+...+\theta_nx_n(m)
   $$

   将上面的式子由矩阵来表达：
   $$
   h_\theta(X)=X\theta
   $$
   其中，$X$为$m\times(n+1)$维的矩阵，$\theta$为$(n+1)\times1$的向量，那么假设函数$h_\theta(X)$就是$m\times1$的向量。

   损失函数的矩阵表达式为：
   $$
   J(θ)=\frac{1}{m}(X\theta−Y)^T(X\theta−Y)
   $$
   其中$Y$是样本的输出向量，维度为$m\times1$。

2. 算法相关参数初始化: $\theta$向量可以初始化为默认值$\mathbf{0}$，或者调优后的值。算法终止距离$\varepsilon$，步长$\alpha$和2.3.3.1比没有变化。

3. 算法过程：

   1)确定当前位置损失函数的梯度，对于$\theta$向量,其梯度表达式如下：
   $$
   \frac{\partial J(\theta)}{\partial \theta}
   $$
   2)用步长乘以损失函数的梯度，得到当前位置下降的距离，即$\alpha\frac{\partial J(\theta)}{\partial \theta}$对应于前面登山例子中的某一步。

   3)确定$\theta$向量里面每个值的梯度下降距离都小于$\varepsilon$，如果小于$\varepsilon$则算法终止，当前$\theta$向量即为最终参数结果。否则进入步骤4.

   4)更新$\theta$向量，其更新表达式如下。更新完毕后继续转入步骤1
   $$
   \theta=\theta−\frac{\partial J(θ)}{\partial \theta}
   $$
   还是用线性回归的例子来描述具体的算法过程。

   损失函数对于$\theta$向量的偏导数计算如下：
   $$
   \frac{\partial J(θ)}{\partial \theta}=X^T(X\theta−Y)
   $$
   步骤4中$\theta$向量的更新表达式如下：$\theta=\theta−\alpha X^T(X\theta−Y)$

   对于2.3.3.1的代数法，可以看到矩阵法要简洁很多。这里面用到了矩阵求导链式法则，和两个矩阵求导的公式。

   公式1：$\frac{\partial (XX^T)}{\partial X}=2X$

   公式2：$\frac{\partial X\theta}{\partial \theta}=X^T$

   如果需要熟悉矩阵求导建议参考张贤达的《矩阵分析与应用》一书。

###  梯度下降的算法调优

在使用梯度下降时，需要进行调优。哪些地方需要调优呢？

1. 算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上步长取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较优值。

2. 算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，最后选择损失函数最小化的初值。

3. 归一化。由于样本的不同特征有不一样的取值范围，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对每个特征$x$，求出它的期望$\mu(x)$和标准差$\sigma(x)$，然后转化为：
   $$
   \frac{x−\mu(x)}{\sigma(x)}
   $$
   这样特征的新期望为0，新方差为1，迭代次数可以大大较少。

###  梯度下降法大家族(BGD，SGD，MBGD)

------

#### 1.  批量梯度下降法(Batch Gradient Descent)

批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新，这个方法对应于前面2.3.3.1的线性回归的梯度下降算法，也就是说2.3.3.1的梯度下降算法就是批量梯度下降法。　　
$$
\theta_i=\theta_i−\alpha\sum_{j=1}^m(h_\theta(x_0(j),x_1(j),...,x_n(j))−y_j)\frac{\partial{h_\theta}}{\partial \theta_i}
$$
由于我们有$m$个样本，这里求梯度的时候就用了所有$m$个样本的梯度数据。

#### 2.  随机梯度下降法(Stochastic Gradient Descent)

随机梯度下降法，其实和批量梯度下降法原理类似，区别在于求梯度时没有用所有的$m$个样本的数据，而是仅选取一个样本$j$来求梯度。对应的更新公式是：
$$
\theta_i=\theta_i−\alpha(h_\theta(x_0(j),x_1(j),...,x_n(j))−y_j)\frac{\partial{h_\theta}}{\partial \theta_i}
$$
随机梯度下降法，和2.4.1的批量梯度下降法是两个极端，一个采用所有数据来梯度下降，另一个只用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。

那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这就是小批量梯度下降法。

#### 3.  小批量梯度下降法(Mini-batch Gradient Descent)

小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于$m$个样本，我们采用其中的$k$个样本来迭代，$1<k<m$。一般可以取$k=10$，当然根据样本的数量，可以调整这个$k$值。对应的更新公式是：
$$
\theta_i=\theta_i−\alpha\sum_{j=1}^k(h_\theta(x_0(j),x_1(j),...,x_n(j))−y_j)\frac{\partial{h_\theta}}{\partial \theta_i}
$$

###  梯度下降法和其他无约束优化算法的比较

------

除了梯度下降外，机器学习中的无约束优化算法还有前面提到的最小二乘法、牛顿法和拟牛顿法。

和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法就需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，这种情况使用迭代的梯度下降法就比较有优势。

梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的Hessen矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快，但是每次迭代的时间比梯度下降法长。

## 牛顿法

​	除了前面说的梯度下降法，牛顿法也是机器学习中用的比较多的一种优化算法。牛顿法的基本思想是利用迭代点$ x_k$处的一阶导数(梯度)和二阶导数(Hessen矩阵)对目标函数进行二次函数近似，然后把二次模型的极小点作为新的迭代点，并不断重复这一过程，直至求得满足精度的近似极小值。牛顿法的速度相当快，而且能高度逼近最优值。

​	在介绍牛顿法之前先介绍一下泰勒公式。泰勒公式就是用多项式函数去逼近光滑函数，是将一个在$x=x_0$处具有n阶导数的函数$f(x)$利用关于$(x−x_0)$的n次多项式来逼近函数的方法。  若函数$f(x)$在包含$x_0$的某个闭区间$[a,b]$上具有n阶导数，且在开区间$(a,b)$上具有$(n+1)$阶导数，则对闭区间$[a,b]$上任意一点$x$，成立下式：
$$
f(x) = f(x_k)+(x-x_k)f'(x_k)+\frac{1}{2!}(x-x_k)^2f''(x_k)+o^n
$$
​	牛顿法的最初提出是用来求解方程的根的。我们假设点$x_*$为函数$f(x)$的根，那么有$f(x_*)=0$。现在我们把函数$f(x)$在点$x_k$处一阶泰勒展开有：
$$
f(x) = f(x_k) + f'(x_k)(x-x_k)
$$
​	那么假设点$x_{k+1}$为该方程的根，则有：
$$
f(x_{k+1}) = f(x_k) + f'(x_k)(x_{k+1} - x_k) = 0
$$
​	那么就可以得到：
$$
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_{k})}
$$
​	图示如下：

![202-ml-basics-08](202-ml-basics-08/202-ml-basics-08-02-youhua.png)

​	对于最优化问题，其极值点处有一个特性就是在极值点处函数的一阶导数为0。因此我们可以在一阶导数处利用牛顿法通过迭代的方式来求得最优解，即相当于求一阶导数对应函数的根。 在现有极小点估计值的附近对$f(x)$做二阶泰勒展开，进而找到极小点的下一个估计值。设$x_k$为当前的极小点估计值，则：  
$$
f(x)=f(x_k)+f'(x_k)(x-x_k)+\frac{1}{2}f''(x_k)(x-x_k)^2
$$
​	对上式求导并令其为0，则为：
$$
f'(x_k)+f''(x_k)(x-x_k)=0
$$
​	既得到牛顿法的更新公式：
$$
x=x_k-\frac{f'(x_k)}{f''(x_k)}
$$
​	 这样我们就得到了一个不断更新$x$迭代求得最优解的方法。这个也很好理解，假设我们上面的第一张图的曲线表示的是函数$f(x)$一阶导数的曲线，那么其二阶导数就是一阶导数对应函数在某点的斜率，也就是那条切线的斜率，那么该公式就和上面求根的公式本质是一样的。  我们这里讨论的都是在低维度的情形下，那么对于高维函数，其二阶导数就变为了一个海森矩阵，记为$H(x)=[\frac{\delta ^2f}{\delta x_i\delta x_j}]$，那么迭代公式就变为了 :
$$
x^{k+1} = x^k - H_k^{-1}f'_k
$$
​	我们可以看到，当$H_k$为正定（$H_k^{-1}$ 也为正定）的时候，可以保证牛顿法的搜索方向是向下搜索的。

​	牛顿法求最优值的步骤如下：

1. 随机选取起始点$x_0$；  
2. 计算目标函数$f(x)$在该点$x^k$的一阶导数和海森矩阵；  
3. 依据迭代公式$x^{k+1} = x^k - H_k^{-1}f'_k​$更新$x​$值  如果$E(f(x_{k+1}) - f(x_k)) < \epsilon​$则收敛返回，否则继续步骤2,3直至收敛 。

​        我们可以看到，当我们的特征特别多的时候，求海森矩阵的逆的运算量是非常大且慢的，这对于在实际应用中是不可忍受的，因此我们想能否用一个矩阵来代替海森矩阵的逆呢，这就是拟牛顿法的基本思路。

## 拟牛顿法

​	因为我们要选择一个矩阵来代替海森矩阵的逆，那么我们首先要研究一下海森矩阵需要具有什么样的特征才能保证牛顿法成功的应用。通过上面的描述我们知道：
$$
f'(x^{k+1}) = f'(x^k)+H_k(x^{k+1}-x^k) \Rightarrow\\H_K^{-1}(f'(x^{k+1})-f'(x_k)) =x^{k+1}-x^k
$$
​	上式我们称之为拟牛顿条件。  因此，对于我们所选择的替代矩阵$G_k$，需要满足两个条件：

1. 拟牛顿条件，即$G_k(f′(x_k+1)−f′(x_k))=x_k+1−x_k$；
2. 要保证$G_k$为正定矩阵，这是因为只有正定才能保证牛顿法的搜索方向是向下搜索的。

​        假设$y_k=f′(x_k+1)−f′(x_k)，δ_k=x_k+1−x_k$，因为每次迭代我们都需要更新替代矩阵$G_k$，下面介绍一种常用的迭代算法DFP(Davidon-Fletcher-Powell) 。

### DFP算法

​	DFP算法中选择$G_{k+1}$方法是在每一步迭代中在矩阵$G_k$中加两项附加项构成$G_{k+1}$，即：
$$
G_{k+1} = G_k +P_k + Q_k
$$
​	这时：
$$
G_{k+1}y_k = G_ky_k +P_ky_k + Q_ky_k
$$
​	为了使$G_{k+1}$满足拟牛顿条件，我们可以令$P_ky_k = \delta_k, Q_ky_k = -G_ky_k$，这样我们就得到$G_k$的迭代公式。




## 共轭梯度法

​	共轭梯度方法也是一种迭代方法，理论上只要n步就能找到真解，实际计算中，考虑到舍入误差，一般迭代3n到5n步，每步的运算量相当与矩阵乘向量的运算量，对稀疏矩阵特别有效。共轭梯度法收敛的快慢依赖于系数矩阵的谱分布情况，当特征值比较集中，系数矩阵的条件数很小，共轭梯度方法收敛得就快。“超线性收敛性”告诉我们，实际当中，我们往往需要更少的步数就能得到所需的精度的解。

​	考虑线性对称正定方程组：
$$
Ax = b
$$
​	空间中任意向量x都可以用一组相互共轭且线性无关的基向量表示：
$$
x=\sum_{i=1}^na_ip_i
$$

​	利用共轭性和矩阵的对称正定性我们定义：
$$
\alpha_i = \frac{\langle x,p_i\rangle}{\langle p_i,p_i\rangle}=\frac{x^TAp_i}{p_i^TAp_i}=\frac{b^Tp_i}{p_i^TAp_i},\quad \forall i=0,1,\cdots,n-1
$$
 	假设已经知道了 $p_0,p_1,⋯,p_{i−1}$, 我们记：
$$
p_ {i} = r_{i}-\sum_{k<i}\beta_k p_k
$$
​	在上式两端对$p_k,k<i$， 做内积有：
$$
\beta_k = \frac{\langle p_k,r_i\rangle}{\langle p_k,p_k\rangle}=\frac{p_k^TAr_i}{p_k^TAp_k}\quad \forall k<i
$$
​	也就是说：
$$
p_ {i} = r_{i}-\sum_{k<i}\frac{p_k^TAr_i}{p_k^TAp_k} p_k
$$

​	接下来我们要选择$r_0,r_1,⋯,r_{n−1}$， 这里我们令：
$$
r_i = b-Ax_i,\quad x_i = \sum_{k<i}\alpha_kp_k,\quad i=0,1,\cdots,n-1\\r_{i+1} = b-Ax_{i+1}=b-A(x_i+\alpha_i p_i)=r_i-\alpha_iAp_i
$$
​	以及：
$$
p_k^Tr_i = p_k^T(b-Ax_i)=p_k^Tb-\alpha_kp_k^TAp_k=0,\quad \forall k<i\\
p_k^TAr_i=\frac{1}{\alpha_k}(r_k-r_{k+1})^Tr_i
$$
​	如果$k<i$  :
$$
r_k^Tr_i=(p_k+\sum_{j<i}\beta_jp_j)^Tr_i=0
$$
​	则：
$$
p_k^TAr_i=0,\quad \forall k<i-1,\quad p_{i-1}^TAr_i=-\frac{1}{\alpha_{i-1}}r_i^Tr_i
$$
​	同样：
$$
p_k^TAp_k = p_k^TA(r_k-\sum_{j<k}\beta_jp_j)=p_k^TAr_k=\frac{1}{\alpha_k}r_k^Tr_k
$$
​	则：
$$
p_i = r_i+\frac{r_i^Tr_i}{r_{i-1}^Tr_{i-1}}p_{i-1}
$$


​	化简后的$\alpha_i$：
$$
\alpha_i = \frac{b^Tp_i}{p_i^TAp_i} = \frac{(r_i+Ax_i)^Tp_i}{p_i^TAp_i}=\frac{r_i^Tp_i}{p_i^TAp_i}=\frac{r_i^Tr_i}{p_i^TAp_i}
$$


​	最后我们有如下算法：

1. 初始条件：$x_0=0,r_0=b,p_0=r_0$

2. 迭代：
   $$
   \alpha_i = \frac{r_i^Tr_i}{p_i^TAp_i}\\x_{i+1}=x_i+\alpha_ip_i\\r_{i+1} = r_i-\alpha_iAp_i\\p_{i+1}=p_i+\frac{r_{i+1}^Tr_{i+1}}{r_{i}^Tr_{i}}p_{i}
   $$


## 坐标下降

​	坐标下降法属于一种非梯度优化的方法，它在每步迭代中沿一个坐标的方向进行搜索，通过循环使用不同的坐标方法来达到目标函数的局部极小值。 

​	算法过程：

​	For $i=1$  to $n$：    
$$
x_i  := \mathop{\arg\min}_{\theta} F(x_1,x_2,…,x_{i-1},x_i,x_{i+1},…,x_n)
$$
​	相当于每次迭代都只是更新$x$的一个维度，即把该维度当做变量，剩下的$n-1$个维度当作常量,通过最小化$f(x)$来找到该维度对应的新的值。坐标下降法就是通过迭代地构造序列$x_0,x_1,x_2,…$来求解问题，即最终点收敛到期望的局部极小值点。通过上述操作，显然有： 
$$
f(x^0)\geq f(x^1)\geq f(x^2)\geq\ldots
$$
​	坐标下降法在每次迭代中在当前点处沿一个坐标方向进行一维搜索 ，固定其他的坐标方向，找到一个函数的局部极小值。坐标下降优化方法是一种非梯度优化算法。在整个过程中依次循环使用不同的坐标方向进行迭代，一个周期的一维搜索迭代过程相当于一个梯度迭代。gradient descent 方法是利用目标函数的导数（梯度）来确定搜索方向的，该梯度方向可能不与任何坐标轴平行。而coordinate descent方法是利用当前坐标方向进行搜索，不需要求目标函数的导数，只按照某一坐标方向进行搜索最小值。

​                 

​                    




