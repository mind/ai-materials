FIXME

# 基本概念


##  梯度下降

在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降(Gradient Descent)是最常采用的方法之一，这里就对梯度下降法做一个完整的总结。

### 1 梯度

在微积分里面，对多元函数的参数求偏导数(符号$\partial$)，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数$f(x,y)$，分别对$x,y$求偏导数，求得的梯度向量就是$(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})$，简称$\mathop{grad}f(x,y)$或者$\nabla f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y})|_{(x_0,y_0)}$或者$\nabla f(x_0,y_0)$，如果是3个参数的向量梯度，就是$(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y},\frac{\partial f}{\partial z})$，以此类推。

那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对x于函数$f(x,y)$,在点$(x_0,y_0)$，沿着梯度向量的方向就是的方$(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y})|_{(x_0,y_0)}$向，是$f(x,y)$增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向$-(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y})|_{(x_0,y_0)}$，梯度减少最快，更加容易找到函数的最小值。

### 2 梯度下降与梯度上升

在机器学习算法中，在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。

梯度下降法和梯度上升法是可以互相转化的。比如我们需要求解损失函数$f(\theta)$的最小值，这时我们需要用梯度下降法来迭代求解。但是实际上，我们可以反过来求解损失函数$-f(\theta)$的最大值，这时梯度上升法就派上用场了。

