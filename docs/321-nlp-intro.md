# 自然语言处理概论

## 自然语言处理基本概念

> ​	自然语言处理（NLP）是计算机科学和人工智能领域的一个领域，涉及计算机和人类（自然）语言之间的交互，特别是如何编写计算机来处理和分析大量的自然语言数据。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。 自然语言处理方面的挑战经常涉及语音识别、自然语言理解和自然语言生成。语言是人类区别其他动物的本质特性。
>
> ​	在所有生物中，只有人类才具有语言能力。人类的多种智能都与语言有着密切的关系。人类的逻辑思维以语言为形式，人类的绝大部分知识也是以语言文字的形式记载和流传下来的。因而，它也是人工智能的一个重要，甚至核心部分。用自然语言与计算机进行通信，这是人们长期以来所追求的。因为它既有明显的实际意义，同时也有重要的理论意义：人们可以用自己最习惯的语言来使用计算机，而无需再花大量的时间和精力去学习不很自然和习惯的各种计算机语言；人们也可通过它进一步了解人类的语言能力和智能的机制。实现人机间自然语言通信意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本来表达给定的意图、思想等。前者称为自然语言理解，后者称为自然语言生成。因此，自然语言处理大体包括了自然语言理解和自然语言生成两个部分。历史上对自然语言理解研究得较多，而对自然语言生成研究得较少。但这种状况已有所改变。
>
> ​	无论实现自然语言理解，还是自然语言生成，都远不如人们原来想象的那么简单，而是十分困难的。从现有的理论和技术现状看，通用的、高质量的自然语言处理系统，仍然是较长期的努力目标，但是针对一定应用，具有相当自然语言处理能力的实用系统已经出现，有些已商品化，甚至开始产业化。典型的例子有：多语种数据库和专家系统的自然语言接口、各种机器翻译系统、全文信息检索系统、自动文摘系统等。自然语言处理，即实现人机间自然语言通信，或实现自然语言理解和自然语言生成是十分困难的。造成困难的根本原因是自然语言文本和对话的各个层次上广泛存在的各种各样的歧义性或多义性（ambiguity）。
>
> ​	一个中文文本从形式上看是由汉字（包括标点符号等）组成的一个字符串。由字可组成词，由词可组成词组，由词组可组成句子，进而由一些句子组成段、节、章、篇。无论在上述的各种层次：字（符）、词、词组、句子、段，……还是在下一层次向上一层次转变中都存在着歧义和多义现象，即形式上一样的一段字符串，在不同的场景或不同的语境下，可以理解成不同的词串、词组串等，并有不同的意义。一般情况下，它们中的大多数都是可以根据相应的语境和场景的规定而得到解决的。也就是说，从总体上说，并不存在歧义。这也就是我们平时并不感到自然语言歧义，和能用自然语言进行正确交流的原因。但是一方面，我们也看到，为了消解歧义，是需要极其大量的知识和进行推理的。如何将这些知识较完整地加以收集和整理出来；又如何找到合适的形式，将它们存入计算机系统中去；以及如何有效地利用它们来消除歧义，都是工作量极大且十分困难的工作。这不是少数人短时期内可以完成的，还有待长期的、系统的工作。以上说的是，一个中文文本或一个汉字（含标点符号等）串可能有多个含义。它是自然语言理解中的主要困难和障碍。反过来，一个相同或相近的意义同样可以用多个中文文本或多个汉字串来表示。因此，自然语言的形式（字符串）与其意义之间是一种多对多的关系。其实这也正是自然语言的魅力所在。但从计算机处理的角度看，我们必须消除歧义，而且有人认为它正是自然语言理解中的中心问题，即要把带有潜在歧义的自然语言输入转换成某种无歧义的计算机内部表示。歧义现象的广泛存在使得消除它们需要大量的知识和推理，这就给基于语言学的方法、基于知识的方法带来了巨大的困难，因而以这些方法为主流的自然语言处理研究几十年来一方面在理论和方法方面取得了很多成就，但在能处理大规模真实文本的系统研制方面，成绩并不显著。研制的一些系统大多数是小规模的、研究性的演示系统。
>
> ​	目前存在的问题有两个方面：一方面，迄今为止的语法都限于分析一个孤立的句子，上下文关系和谈话环境对本句的约束和影响还缺乏系统的研究，因此分析歧义、词语省略、代词所指、同一句话在不同场合或由不同的人说出来所具有的不同含义等问题，尚无明确规律可循，需要加强语用学的研究才能逐步解决。另一方面，人理解一个句子不是单凭语法，还运用了大量的有关知识，包括生活知识和专门知识，这些知识无法全部贮存在计算机里。因此一个书面理解系统只能建立在有限的词汇、句型和特定的主题范围内；计算机的贮存量和运转速度大大提高之后，才有可能适当扩大范围。
>
> ​	以上存在的问题成为自然语言理解在机器翻译应用中的主要难题，这也就是当今机器翻译系统的译文质量离理想目标仍相差甚远的原因之一；而译文质量是机译系统成败的关键。中国数学家、语言学家周海中教授曾在经典论文《机器翻译五十年》中指出：要提高机译的质量，首先要解决的是语言本身问题而不是程序设计问题；单靠若干程序来做机译系统，肯定是无法提高机译质量的；另外在人类尚未明了大脑是如何进行语言的模糊识别和逻辑判断的情况下，机译要想达到“信、达、雅”的程度是不可能的。、
>
> *内容引自百度百科*

## 自然语言处理的发展

> ​	最早的自然语言理解方面的研究工作是机器翻译。1949年，美国人威弗首先提出了机器翻译设计方案。20世纪60年代，国外对机器翻译曾有大规模的研究工作，耗费了巨额费用，但人们当时显然是低估了自然语言的复杂性，语言处理的理论和技术均不成热，所以进展不大。主要的做法是存储两种语言的单词、短语对应译法的大辞典，翻译时一一对应，技术上只是调整语言的同条顺序。但日常生活中语言的翻译远不是如此简单，很多时候还要参考某句话前后的意思。
>
> ​	大约90年代开始，自然语言处理领域发生了巨大的变化。这种变化的两个明显的特征是：
>
> （1）对系统输入，要求研制的自然语言处理系统能处理大规模的真实文本，而不是如以前的研究性系统那样，只能处理很少的词条和典型句子。只有这样，研制的系统才有真正的实用价值。
>
> （2）对系统的输出，鉴于真实地理解自然语言是十分困难的，对系统并不要求能对自然语言文本进行深层的理解，但要能从中抽取有用的信息。例如，对自然语言文本进行自动地提取索引词，过滤，检索，自动提取重要信息，进行自动摘要等等。
>
> ​	同时，由于强调了“大规模”，强调了“真实文本”，下面两方面的基础性工作也得到了重视和加强。
>
> （1）大规模真实语料库的研制。大规模的经过不同深度加工的真实文本的语料库，是研究自然语言统计性质的基础。没有它们，统计方法只能是无源之水。
>
> （2）大规模、信息丰富的词典的编制工作。规模为几万，十几万，甚至几十万词，含有丰富的信息（如包含词的搭配信息）的计算机可用词典对自然语言处理的重要性是很明显的。
>
> *内容引自百度百科*

## 自然语言处理的基本任务

​	下面是一些在自然语言处理中最常见的研究任务的列表。请注意，其中一些任务具有直接的实际应用程序，而另一些任务则更常见的是用于帮助解决更大任务的子任务。 

### 语法

* 分词
* 词性标注
* 句法分析
* 主题词提取
* 信息抽取
* ...
### 语义
* 词汇语义学
* 机器翻译
* 命名实体识别
* 问答系统
* 自然语言理解
* 关系抽取
* 情感分析
* 主题分类和识别
* 词义消歧
* ...

## 自然语言处理的研究现状

> 词性标注和句法分析联合建模：研究者们发现，由于词性标注和句法分析紧密相关，词性标注和句法分析联合建模可以同时显著提高两个任务准确率。
>
> 异构数据融合：汉语数据目前存在多个人工标注数据，然而不同数据遵守不同的标注规范，因此称为多源异构数据。近年来，学者们就如何利用多源异构数据提高模型准确率，提出了很多有效的方法，如基于指导特征的方法、基于双序列标注的方法、以及基于神经网络共享表示的方法。
>
> 基于深度学习的方法：传统方法的特征抽取过程主要是将固定上下文窗口的词进行人工组合，而深度学习方法能够自动利用非线性激活函数完成这一目标。进一步，如果结合循环神经网络如双向 LSTM，则抽取到的信息不再受到固定窗口的约束，而是考虑整个句子。
>
> （1） 已开发完成一批颇具影响的语言资源库， 部分技术已达到或 基本达到实用化程度， 并在实际应用中发挥着巨大作用。 例如， 北京大 学语料库和综合型语言知识库 、 HowNet 、 LDC 语言资源等， 以及汉字输入、编辑、 排版、 文字识别、 电子词典、 语音合成和机器翻译系 统、 搜索引擎（Google， 百度） 等。 值得提及的是， 中文信息处理在60多年的辉煌历史中产生了一大批 令人鼓舞的成果， 除了上面提到的已经开发完成的汉语信息处理用语言 资源库和部分实用化汉语信息处理技术外， 在语文现代化和汉字信息处 理技术等方面也取得了丰硕成果， 有关规范化汉字、 汉语拼音和普通话 的一系列国家法规、 标准和规范已经形成； 汉字信息处理技术已达到实 用化水平， 并在实际应用中日趋成熟； 中文信息处理的国内外学术交流 与合作环境已经建立， 中文信息处理正在世界范围内迎来空前繁荣时期。
>
> （2） 许多新的研究方向不断出现。 正如我们前面指出的， 受实际 应用的驱动， 自然语言处理技术不断与新的相关技术相结合， 用于研究 和开发越来越多的实用技术。 例如， 网络内容管理、 网络信息监控和有 害信息过滤等， 这些研究不仅与自然语言处理技术密切相关， 而且涉及 图像理解、 情感计算和网络技术等多种相关技术。 而语音自动翻译则是 涉及语音识别、 机器翻译、 语音合成和通信等多种技术的综合集成技 术。 语音自动文摘、 语音检索和基于图像内容及文字说明的图像理解技 术研究等， 都是集自然语言处理技术和语音技术、 图像技术等于一体的 综合应用技术。 对于这些新的任务， 研究刚刚开始或者仅处于非常初步 的探索阶段， 离问题的最终解决和达到实用化目标还有相当遥远的路程 要走。 
>
> （3） 许多理论问题尚未得到根本性的解决。尽管许多理论模型在自然语言处理研究中发挥着重要作用， 并且很多方法已经得到实际应 用， 如上下文无关文法、 HMM、 噪声信道模型等， 但是，许多重要的 问题仍未得到彻底、 有效的解决， 如语义理解问题、 句法分析问题、 指代歧义消解问题、 汉语自动分词中的未登录词识别问题等。 纵观整个自然语言处理领域， 尚未建立起一套完整、 系统的理论 框架体系。 许多理论研究仍处于盲目的探索阶段， 如尝试一些新的机器 学习方法或未曾使用过的数学模型， 这些尝试和实验带有很强的主观性 和盲目性。 在技术实现上， 许多改进往往仅限于对一些边角问题的修修 补补， 或者只是针对特定条件下一些具体问题的处理， 未能从根本上建 立一套广泛适用的、 鲁棒的处理策略。 总之， 面对自然语言问题的复杂 性和多变性， 现有的理论模型和方法还远远不够， 有待于进一步改进和 完善， 并期待着新的更有效的理论模型和方法的出现。 当然我们不能忘记， 自然语言处理毕竟是认知科学、 语言学和计算 机科学等多学科交叉的复杂问题， 当我们从外层（或表层） 研究语言理 解的理论方法和数学模型的同时， 不应该忽略从内层揭示人类理解语言 机制的秘密， 从人类认知机理和智能的本质上为自然语言处理寻求依 据。 综上所述， 自然语言处理研究已经取得了丰硕成果， 同时也面临着 许多新的挑战。 无论如何， 我们在评价任何一门学科和技术的时候， 既 不应该因为它所取得的成绩而忽略了问题的存在， 也不应该因为问题的 存在而全盘否定这门学科的发展。 对于评价自然语言处理这门学科更是 如此， 因为实际上对于自然语言处理的很多问题， 具有高度智慧的人类 本身解决起来都不能达到非常准确、 满意的程度， 甚至无法清楚地知道 人脑处理这些问题的具体过程， 那么， 在目前对自然语言处理的一些具 体技术提出过高的要求显然没有太多的道理， 给予太多的批评和指责也 是不公正的。 比如说， 在现阶段过高地要求机器翻译系统的译文质量和 信息抽取系统的准确率等， 都是不现实的。 相反， 这些技术在实际应用 中已经在一定程度上为我们提供了很大的帮助和便利。 当然， 我们并不 是不允许人们对某一项技术提出更高的要求和希望， 重要的是应该如何 建立有效的理论模型和实现方法。 这也是自然语言处理这门学科所面临 的问题和挑战。    
>

# 自然语言处理中的概率论

## 概率论

> ​	概率亦称“或然率”、“机率”。它反映随机事件出现的可能性大小的量度。随机事件是指在相同条件下，可能出现也可能不出现的事件。例如，从一批有正品和次品的商品中，随意抽取一件，“抽得的是正品”就是一个随机事件。设对某一随机现象进行了n次试验与观察，其中A事件出现了m次，即其出现的频率为m/n。经过大量反复试验，常有m/n越来越接近于某个确定的常数。该常数即为事件A出现的概率，常用P (A) 表示。 
>

## 条件概率

> ​	条件概率是指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：P（A|B），读作“在B的条件下A的概率”。条件概率可以用决策树进行计算。条件概率的谬论是假设 P(A|B) 大致等于 P(B|A)。 若只有两个事件A，B，那么：$P(A|B)=\dfrac{P(A|B)}{P(B)}$ 

## 先验概率和后验概率
​	先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为"由因求果"问题中的"因"出现的概率。 后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的"果"。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础。事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。 

​	举一个简单的例子：一口袋里有3只红球、2只白球，采用不放回方式摸取，求：

​	⑴ 第一次摸到红球（记作A）的概率；

​	⑵ 第二次摸到红球（记作B）的概率；

​	⑶ 已知第二次摸到了红球，求第一次摸到的是红球的概率。

​	解：

​	⑴ P(A)=3/5，这就是验前概率；

​	⑵ P(B)=P(A)P(B|A)+P($A^{-1}$)P(B|$A^{-1}$)=3/5

​	⑶ P(A|B)=P(A)P(B|A)/P(B)=1/2，这就是后验概率。

## 贝叶斯定理

​	贝叶斯定理是关于随机事件A和B的条件概率的一则定理。其中P(A|B)是在B发生的情况下A发生的可能性。
	贝叶斯公式：
$$
P(A|B) = {P(A)P(B|A) \over P(B)}
$$
​	假设$Ai$是事件集合里的部分集合，对于任意的$A_i$，贝叶斯定理可用下式表示：  
$$
P(A_i|B) ={ P(B|A_i)P(A_i)\over \sum_jP(B|Aj)P(Aj)}
$$
​	公式中，事件$Ai$的概率为$P(A_i)$，事件$Ai$已发生条件下事件B的概率为$P(B│A_i)$，事件B发生条件下事件$A_i$的概率为$P(A_i│B)$。 

​	举例：现分别有 A、B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，现已知从这两个容器里任意抽出了一个球，问这个球是红球且来自容器 A 的概率是多少?

​	假设已经抽出红球为事件 B，选中容器 A 为事件 A，则有：P(B) = 8/20，P(A) = 1/2，P(B|A) = 7/10，按照公式，则有：P(A|B) = (7/10)*(1/2) / (8/20) = 0.875

## 最大似然估计

​	似然（likelihood）对于这个函数：$P(x|θ)$，输入有两个：$x$表示某一个具体的数据，$θ$表示模型的参数。如果$θ$是已知确定的，$x$是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点x，其出现概率是多少。如果x是已知确定的，$θ$是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现x这个样本点的概率是多少。最大似然估计就是找到参数$θ$的一个估计值，使得当前样本出现的可能性最大。

​	假设有一组独立同分布的随机变量$X$，给定一个概率分布$D$，假设其概率密度函数为$f$，以及一个分布的参数$θ$，从这组样本中抽出$x1,x2,⋯,xn$，那么通过参数θ的模型$f$产生上面样本的概率为：  
$$
f(x_1,x_2,\cdots,x_n|\theta) = f(x_1|\theta) \times f(x_2|\theta) \times \cdots f(x_n|\theta)
$$
​	最大似然估计会寻找关于$θ$ 的最可能的值，即在所有可能的 $θ$ 取值中，寻找一个值使这个采样的“可能性”最大化，因为是”模型已定，参数未知”，此时我们是根据样本采样$x_1,x_2,⋯,x_n$取估计参数$θ$，定义似然函数为： 
$$
L(\theta|x_1,x_2,\cdots,x_n)=f(x_1,x_2,\cdots,x_n|\theta) = \prod f(x_i | \theta)
$$
​	实际使用中，因为$f(x_i|θ)$一般比较小，而且$n$往往会比较大，连乘容易造成浮点运算下溢。所以一般我们用对数似然函数： 
$$
\ln L(\theta|x_1,x_2,\cdots,x_n) = \sum_{i=1}^n f(x_i | \theta)
$$

$$
\widehat l= \frac{1}{n} \ln L
$$

​	那最终$θ$的估计值为： 
$$
\begin{equation}
\widehat{\theta}_{MLE} =\mathop{\arg\max}_{\theta} \widehat l(\theta|x_1,x_2,\cdots,x_n)
\end{equation}
$$
### 实例

​	假如有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。我 们想知道罐中白球和黑球的比例，但我们不能把罐中的球全部拿出来数。现在我们可以每次任意从已经摇匀的罐中拿一个球出来，记录球的颜色，然后把拿出来的球 再放回罐中。这个过程可以重复，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少？  估计大家很快能反应出来答案是70%。但是如果让推导一下具体过程呢？  我们假设罐中白球的比例是$p$，那么黑球的比例就是$1−p$。因为每抽一个球出来，在记录颜色之后，我们把抽出的球放回了罐中并摇匀，所以每次抽出来的球的颜 色服从同一独立分布。这里我们把一次抽出来球的颜色称为一次抽样。题目中在一百次抽样中，七十次是白球的概率是$p(x|θ)$，这里xx是所有的抽样，$θ$是所给出的模型参数，表示每次抽出来的球是白色的概率为$p$。 

​	先写出似然函数：  
$$
\begin{align}
 p(x|\theta) & = p(x_1,x_2,\cdots,x_n| \theta) \\
 & = p(x_1 | \theta) \cdot p(x_2 | \theta) \cdots p(x_n | \theta) \\
 & = p^{70}(1-p)^{30}
 \end{align}
$$
​	接下来对似然函数对数化：
$$
\begin{align}
\ln p(x|\theta) &= \ln (p^{70}(1-p)^{30}) \\
& =70 \ln p + 30\ln (1-p)
 \end{align}
$$
 	然后求似然方程： 
$$
\begin{align}
\ln' p(x|\theta) &= \frac{70}{p} - \frac{30}{1-p} \\
 \end{align}
$$
​	最后求解似然方程，得：$p=0.7  $

## 随机变量

​	随机变量（random variable）表示随机试验各种结果的实值单值函数。随机事件不论与数量是否直接有关，都可以数量化，即都能用数量化的方式表达。随机事件数量化的好处是可以用数学分析的方法来研究随机现象。例如某一时间内公共汽车站等车乘客人数，电话交换台在一定时间内收到的呼叫次数，灯泡的寿命等等，都是随机变量的实例。

​	随机变量的类型：离散型随机变量、连续型随机变量。如果随机变量X的取值为有限个，或者可数个，则称$X$为离散型随机变量。 $X$的概率函数为：    
$$
P(X=x_k)=p_k,k=1,2,3...
$$
​	对于随机变量$X$的分布函数$F(x)$，若存在非负的函数$f(x)$，使对于任意实数x有：$F(x)=∫_{+∞}^{−∞}f(t)dt$。则称$X$为连续型随机变量，$f(x)$为$X$的概率密度函数。 

## 联合概率分布、条件概率分布



## 二项式分布

​	关系：如果试验E只有两个可能的结果：$A$或者$A¯$，$P(A)=p,0<p<1$,将E独立的重复进行n次，想了解n重贝努力试验中A发生的次数的统计规律，就是二项分布。定义：若$X$的概率分布律为$P(X=k)=C^k_np^k(1−p)^{n−k},k=0,1,2...,n>=1,0<p<1 $，就称$X$服从参数$n，p$的二项分布，记为$X∼B(n,p)$。 

## 期望和方差


# 自然语言处理中的信息论
## 熵
## 联合熵
## 条件熵	
## 互信息
## 相对熵
## 交叉熵
## 困惑度
## 噪声


# 语料库与语言知识库
## 命题库
## 名词化树库
## 语篇库
## wortnet
## framenet
## EDR
## 知网

