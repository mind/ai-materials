<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>聚类算法 - ai2.1 课程大纲与知识点</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "\u805a\u7c7b\u7b97\u6cd5";
    var mkdocs_page_input_path = "202-ml-basics/202-ml-basics-06.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> ai2.1 课程大纲与知识点</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../..">前言</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">机器学习基础</span>
    <ul class="subnav">
                <li class="">
                    
    <span class="caption-text">机器学习入门</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../201-ml-intro/">机器学习入门</a>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">基本概念</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-01/">机器学习简介</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-02/">线性回归</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-03/">logitstic回归</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-04/">损失</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-05/">梯度下降</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-06/">正则化/weight decay</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">应用技巧</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-skills-07/">非均衡数据</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-skills-08/">Hard-Example</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-skills-09/">Early Stop</a>
                </li>
                <li class=" current">
                    
    <span class="caption-text">机器学习基础算法</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-01/">支持向量机</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-02/">决策树</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-03/">随机森林</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-04/">GBDT</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-05/">PCA</a>
                </li>
                <li class="toctree-l3 current">
                    
    <a class="current" href="./">聚类算法</a>
    <ul class="subnav">
            
    <li class="toctree-l4"><a href="#_1">聚类</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#1">1 概述</a></li>
        
            <li><a class="toctree-l5" href="#2">2 距离度量</a></li>
        
            <li><a class="toctree-l5" href="#3">3 常见聚类算法</a></li>
        
            <li><a class="toctree-l5" href="#4">4 聚类性能评估</a></li>
        
            <li><a class="toctree-l5" href="#46_calinski-harabaz">4.6 Calinski-Harabaz指数</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../404.md">吸引力传播</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-07/">集成学习</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-08/">优化算法</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">机器学习算法进阶</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../203-ml-advance/203-ml-advance-03/">异常检测</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../404.md">最大期望算法</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">深度学习入门</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/">深度学习入门</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-01/">深度学习基础</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-02/">深度学习历史发展</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-03/">感知器与优化规则</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-04/">异或问题</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-05/">神经网络简介</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-06/">全连接网络</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">深度学习基础算法</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../212-dl-basics/">深度学习基础算法</a>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">基础算法</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-01/">神经网络前向传播</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-02/">神经网络拟合数据</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-03/">神经网络反向传播</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">调参</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-04/">过拟合与欠拟合</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-05/">学习率/lr decay</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-06/">优化算法：动量</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-07/">正则化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-08/">进一步讨论损失函数</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-09/">进一步讨论激活函数</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-10/">参数的初始化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-11/">batch norm</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-12/">dropout</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-13/">蒸馏法/transfer learning</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-14/">深度置信网络</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-15/">自编码器</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">视觉</span>
    <ul class="subnav">
                <li class="">
                    
    <span class="caption-text">计算机视觉基础</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <span class="caption-text">视觉介绍</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-01/">动物视觉介绍</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-02/">计算机视觉介绍</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-03/">视觉系统</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-04/">视觉认知</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">数字图像基础</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-05/">图像信号的数学表示</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-06/">图像的采样和量化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-07/">像素连通性</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-08/">rgb/bgr/lab/yuv/hsv/cmy及换算公式</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-09/">视频压缩与图像显示</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-10/">图像的线性系统理论</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">常用CV算法</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <span class="caption-text">图像处理与变换</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-01/">二维傅立叶变换及其基本性质</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-02/">快速傅立叶变换</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-03/">离散小波变换</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-04/">象素间的连通性</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-05/">灰度/彩色直方图</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-06/">图像空域滤波技术</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-07/">图像频域滤波</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-08/">多光谱图像处理</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-09/">颜色特征的图像检索</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-10/">图像金字塔</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-11/">二值化/大津算法/开闭操作</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-12/">边缘检测：sobel/laplance/canny</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-13/">泛洪填充</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">视觉几何基础</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-14/">坐标系与坐标变化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-15/">多视角</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-16/">标定</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-17/">双目与景深</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-18/">哈夫变换</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-19/">姿态与空间重建/多图使用</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">视觉认知模型</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-20/">微分算子</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-21/">阈值分割</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-22/">区域生长</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-23/">评价测度</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-24/">图搜索</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-25/">动态规划</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-26/">灰度共生矩阵</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-27/">基于模型的纹理分析</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-28/">运动估计简介/跟踪</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-29/">光流法</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">识别与高层认知</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-30/">图像的特征点</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-31/">Harris算法</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-32/">SIFT/SURF算法</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-33/">HOG算子检测</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-34/">距离</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-35/">统计分类方法</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-36/">马尔科夫随机场</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-37/">条件随机场</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-38/">模板匹配</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-39/">目标匹配</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-40/">特征内容匹配</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-41/">Marr视觉计算理论</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">应用</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-42/">多序列的三维重建</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-43/">数字图书馆藏查询</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">OpenCV</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../303-cv-opencv/303-cv-opencv-01/">opencv简介</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../303-cv-opencv/303-cv-opencv-02/">openCV安装</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../303-cv-opencv/303-cv-opencv-03/">图片存取与显示</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../303-cv-opencv/303-cv-opencv-04/">颜色转换</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../303-cv-opencv/303-cv-opencv-05/">各经典算法的api及文档导读</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">计算机视觉与神经网络</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <span class="caption-text">卷积神经网络</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-01/">卷积计算</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-02/">初始化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-03/">传统卷积使用</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-04/">卷积神经网络介绍</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-05/">池化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-06/">特征的使用</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-07/">业务网络设计</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-08/">分类器的概念</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">卷积神经网络案例</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-09/">VGG</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-10/">GoogleLeNet</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-11/">resnet</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-12/">densenet</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-13/">nasnet</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-14/">se-net</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-15/">MobileNet v1/v2</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-16/">已被证明有效的基础模块</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">卷积神经网络应用</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-17/">分类</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-18/">检测</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-19/">分割</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-20/">人脸</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-21/">各种娱乐项目（style transher/deepdream/nima/gan）</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-22/">backbone</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../321-nlp-intro/">自然语言处理NLP</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">附录</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../notation/">符号约定</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">ai2.1 课程大纲与知识点</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>机器学习基础算法 &raquo;</li>
        
      
        
          <li>机器学习基础 &raquo;</li>
        
      
    
    <li>聚类算法</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="_1">聚类<a class="headerlink" href="#_1" title="Permanent link">#</a></h1>
<h2 id="1">1 概述<a class="headerlink" href="#1" title="Permanent link">#</a></h2>
<p>ＦＩＸＭＥ</p>
<p>聚类就是对大量未标注的数据集，按照数据内部存在的数据结构特征将数据集划分为若干类，使类内的数据比较相似，类间的数据比较相异，属于无监督学习方法。
聚类算法的重点是计算数据之间的相似度，即数据间的距离。</p>
<h2 id="2">2 距离度量<a class="headerlink" href="#2" title="Permanent link">#</a></h2>
<p>ＦＩＸＭＥ</p>
<p>对函数$dist(\cdot,\cdot)$，若它是一个“距离度量”(distance measure)，则需满足一些基本性质:</p>
<p>非负性: $dist(x_i,x_j) \geq 0$;
同一性: $dist(x_i,x_j) = 0$当且仅当$x_i=x_j$;
对称性: $dist(x_i,x_j) = dist(x_j,x_i)$;
直递性: $dist(x_i,x_j) \leq dist(x_i,x_k) + dist(x_k,x_j)$</p>
<p>给定样本$x_i = (x_{i1};x_{i2};...;x_{in})$与$x_j=(x_{j1};x_{j2};...;x_{jn})$，最常用的距离度量是闵可夫斯基距离(Minkowski distance)
<script type="math/tex; mode=display">
dist_{mk}(x_i,x_j)=(\sum \limits_{u=1}^n |x_{iu}-x_{ju}|^p)^{\frac{1}{p}}
</script>
</p>
<p>对$p \geq 1$，闵可夫斯基距离显然满足距离度量的基本性质。</p>
<p>$p=2$时，闵可夫斯基距离就是欧式距离(Euclidean distance)
<script type="math/tex; mode=display">
dist_{ed}(x_i,x_j)=\|x_i-x_j\|_2=\sqrt{\sum \limits_{u=1}^n |x_{iu}-x_{ju}|^2}
</script>
$p=1$时，闵可夫斯基距离就是曼哈顿距离(Manhattan distance)
<script type="math/tex; mode=display">
dist_{man}(x_i,x_j)=\|x_i-x_j\|_1=\sum \limits_{u=1}^n |x_{iu}-x_{ju}|
</script>
</p>
<p>除了闵可夫斯基距离外，还有:</p>
<ol>
<li>
<p>切比雪夫距离:
<script type="math/tex; mode=display">
dist(x_i,x_j) = max(|x_i-x_j|,|y_i-y_j|)
</script>
标准化欧式距离: 由于数据各维度分量的分布不同，那就先将各个分量都"标准化"到均值和方差都相等，然后再使用欧式距离</p>
</li>
<li>
<p>马氏距离:</p>
</li>
</ol>
<p>假设两个样本向量分别为$\vec{x}$和$\vec{y}$，样本分布的均值分别为$\mu(\vec{x})$和$\mu(\vec{y})$，协方差为$S(\vec{x},\vec{y})$，那么马氏距离为
<script type="math/tex; mode=display">
dist(\vec{x},\vec{y})=\sqrt{(\vec{x}-\vec{y})^TS(\vec{x},\vec{y})^{-1}(\vec{x}-\vec{y})}
</script>
</p>
<p>如果样本集合的协方差矩阵是单位矩阵，马氏距离就等于欧式距离。</p>
<ol>
<li>夹角余弦:
<script type="math/tex; mode=display">
dist(x_i,x_j)=\frac{x_i \cdot x_j}{|x_i||x_j|}
</script>
</li>
</ol>
<h2 id="3">3 常见聚类算法<a class="headerlink" href="#3" title="Permanent link">#</a></h2>
<p>ＦＩＸＭＥ</p>
<h3 id="31">3.1 原型聚类<a class="headerlink" href="#31" title="Permanent link">#</a></h3>
<p>原型聚类就是“基于原型的聚类”(Prototype-based Clustering)，"原型"的定义是指样本空间中具有代表性的点，如质心点(即同一类别样本点向量的均值)，基于质心点的聚类算法最经典的就是k-means算法。原型聚类假设聚类结构能通过一组原型刻画，通常算法先对原型进行初始化，然后对原型进行迭代更新求解。采用不同的原型表示、不同的求解方式，将产生不同的算法。下面介绍几种著名的原型聚类算法。</p>
<h4 id="311_k-means">3.1.1 k-means算法<a class="headerlink" href="#311_k-means" title="Permanent link">#</a></h4>
<p>k-means算法是一种使用广泛的最基础的聚类算法，其思想非常简单， 对于给定的样本集，按照样本之间的距离大小，将样本集划分为k个簇，让簇内的点尽量紧密地连在一起，而让簇间的距离尽量大。
我们将k-means算法的思想转化为数学模型，假设簇类划分为$(C_1,C_2,...,C_k)$，则k-means算法的目标是最小化平方误差:
<script type="math/tex; mode=display">
E=\sum \limits_{i=1}^{k} \sum \limits_{x \in C_i} \|x-\mu_i\|_2^2
</script>
其中，$\mu_i=\dfrac{1}{|C_i|}\sum \limits_{x \in C_i}x$，是簇$C_i$的均值向量，有时也称为质心; $|x-\mu_i|^2$代表每个样本点到均值点的欧氏距离。</p>
<p>假设输入样本集$D={x_1,x_2,..,x_m}$，则k-means算法步骤为:
1. 从样本集$D$中随机选择$k$个样本作为初始类别中心${\mu_1,\mu_2,...,\mu_k}$;
2. 对任一样本$x_i$，将其标记为距离最近的类别中心$\mu_j$所属的类j;
3. 更新每个类别中心点$\mu_j$为隶属于该类别的所有样本的均值
4. 循环以上操作，直到达到某个中止条件</p>
<p>中止条件:
- 达到预设的迭代次数
-  最小平均误差MSE小于预设值
- 簇中心点变化率小于预设值</p>
<p>k-means算法的缺点:
- 只针对服从正太分布的簇类样本效果较好，对长条型或者环状型的簇类样本无法适用;
- 参数k需要人为设定
- 初始类别中心的选择对算法迭代次数和结果的影响都非常大</p>
<p><img alt="202-ml-basics-01" src="202-ml-basics-01.png" /></p>
<h3 id="312_affinity_propagation">3.１.2 吸引力传播算法(Affinity Propagation)<a class="headerlink" href="#312_affinity_propagation" title="Permanent link">#</a></h3>
<p>吸引力传播算法是基于数据点间的“信息传递”的一种聚类算法。与k-means算法不同，吸引力传播算法不需要在运行算法之前确定聚类的类别个数。吸引力传播算法寻找的聚类中心点是数据集合中实际存在的点，作为每类的代表。</p>
<p>算法描述：
假设${x_1,x_2,...,x_n}$是数据样本集，数据间没有任何内在结构的假设。令$S$是一个刻画点之间相似度的矩阵，使得$s(i,j) &gt; s(i,k)$当且仅当$x_i$与$x_j$的相似性程度要大于其与$x_k$的相似性。</p>
<p>吸引力传播算法进行交替两个消息传递的步骤，以更新两个矩阵：</p>
<ul>
<li>吸引信息矩阵$R$：$r(i,k)$描述了数据对象$k$适合作为数据对象$i$的聚类中心的程度，表示的是从$i$到$k$的消息；</li>
<li>归属信息矩阵$A$：$a(i,k)$描述了数据对象$i$选择数据对象$k$作为其聚类中心的合适程度，表示从$k$到$i$的消息。</li>
</ul>
<p>两个矩阵$R$和$A$中的全部初始化为０，吸引力传播算法通过以下步骤迭代进行：
1. 吸引信息$r_{t+1}(i,k)$按照
<script type="math/tex; mode=display">
r_{t+1}(i,k)=s(i,k)-\max \limits_{k' \neq k} \{a_t(i,k') + s(i,k')\}
</script>
迭代。其中，$t$是迭代次数。</p>
<ol>
<li>归属信息$a_{t+1}(i,k)$按照</li>
</ol>
<p>
<script type="math/tex; mode=display">
a_{t+1}(i,k) = \min[0,r_t(k,k) + \sum \limits_{i' \notin \{i,k\}} \max \{0,r_t(i',k)\}], i \neq k
</script>
和
<script type="math/tex; mode=display">
a_{t+1}(k,k)=\sum \limits_{i' \neq k} \max\{0,r_t(i',k)\}
</script>
迭代。
3. 对以上步骤进行迭代，直至满足中止条件，算法结束。</p>
<p>为了避免振荡，吸引力传播算法更新信息时引入了衰减系数$\lambda$。每条信息被设置为它前次迭代更新值的$\lambda$倍加上本次信息更新值的$1-\lambda$倍。其中，衰减系数$\lambda$是介于0到1之间的实数。那么第t+1次$r(i,k)$，$a(i,k)$的迭代值：</p>
<p>
<script type="math/tex; mode=display">
r_{t+1}(i,k) = \lambda r_t(i,k) + (1-\lambda)r_{t+1}(i,k)
</script>
</p>
<p>
<script type="math/tex; mode=display">
a_{t+1}(i,k) = \lambda a_t(i,k) + (1-\lambda)a_{t+1}(i,k)
</script>
</p>
<p><img alt="202-ml-basics-02" src="202-ml-basics-02.gif" /></p>
<p>吸引力传播算法的优点：</p>
<ul>
<li>不需要提供聚类的类别数$k$</li>
</ul>
<p>缺点：
- 复杂度太高，计算量太大</p>
<h3 id="32">3.2 密度聚类<a class="headerlink" href="#32" title="Permanent link">#</a></h3>
<h4 id="321_dbscan">3.2.1 DBSCAN<a class="headerlink" href="#321_dbscan" title="Permanent link">#</a></h4>
<p>DBSCAN(Density-Based Spatial Clustering of Applications with  Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用于非凸样本集。下面我们就对DBSCAN算法的原理做一个总结。</p>
<p>DBSCAN是一种基于密度的聚类算法，这类密度聚类算法一般假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的，也就是说，在该类别任意样本周围不远处一定有同类别的样本存在。</p>
<p>通过将紧密相连的样本划为一类，这样就得到了一个聚类类别。通过将所有各组紧密相连的样本划为各个不同的类别，则我们就得到了最终的所有聚类类别结果。</p>
<p>在介绍算法之前先介绍一些概念:</p>
<ul>
<li>$\varepsilon$邻域: 对于样本点$x_i$，和它的距离在$\varepsilon$之内的属于样本集$D$中的点的集合，即$N_{\varepsilon}(x_j)={s_i \in D|dist(x_i,x_j)\leq\varepsilon}$</li>
<li>核心对象: 若$x_j$的$\varepsilon$邻域至少包含$MinPts$个样本，即$|N_{varepsilon}(x_j)| \geq MinPts$，那么$x_j$是一个核心对象。其实也就是在核心对象周围的点相对邻域参数来说是致密的。</li>
<li>密度直达与可达: 直达的意思是点$x_j$位于点$x_i$的$\varepsilon$邻域中，且$x_i$是核心对象。可达的意思是存在这么一个样本序列$p_1,p_2,...,p_n$，$x_j$到$p_1$是直达的，$p_1$到$p_2$是直达的，就这样不断地借着这些样本作为“跳板”，$x_j$可以间接地“跳到”$x_i$。</li>
<li>密度相连: 对于样本点$x_j$和$x_i$若存在点$x_k$使得$x_j$和$x_i$均可由$x_k$密度可达，则称$x_j$和$x_i$密度相连。</li>
</ul>
<p><img alt="202-ml-basics-03" src="202-ml-basics-03.png" /></p>
<p>从上图可以很容易理解上述定义，图中MinPts=5，红色的点都是核心对象，因为其$\varepsilon$邻域至少有5个样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的超球体内，如果不在超球体内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列。在这些密度可达的样本序列的$\varepsilon$邻域内所有的样本相互都是密度相连的。</p>
<p>基于以上概念，DBSCAN所定义的簇类的概念为：由密度可达关系导出的最大的密度相连样本集合。具体来说，给定邻域参数$(\varepsilon,MinPts)$，簇$C \subseteq D$是满足以下性质的非空样本子集:</p>
<p>连接性(connectivity): $x_i \in C$，$x_j \in C \Rightarrow x_i$与$x_j$密度相连</p>
<p>最大性(maximality): $x_i \in C$，$x_j$由$x_i$密度可达$\Rightarrow x_j \in C$</p>
<p>那么，如何从数据集$D$中找出满足以上性质的聚类簇呢? DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止。</p>
<p>于是，DBSCAN算法先任选数据集中的一个核心对象为"种子"(seed)，再由此出发确定相应的聚类簇，</p>
<p>基本上这就是DBSCAN算法的主要内容了，是不是很简单？但是我们还是有三个问题没有考虑。</p>
<p>第一个是一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象的周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。</p>
<p>第二个是距离的度量问题，即如何计算某样本和核心对象样本的距离。在DBSCAN中，一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和KNN分类算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用KD树或者球树来快速的搜索最近邻。</p>
<p>第三种问题比较特殊，某些样本可能到两个核心对象的距离都小于$\varepsilon$，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，此时DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说BDSCAN的算法不是完全稳定的算法。</p>
<p>下面我们对DBSCAN聚类算法的流程做一个总结:</p>
<p>输入: 样本集$D={x_1,x_2,...,x_m}$，邻域参数$(\varepsilon, MinPts)$，样本距离度量方式</p>
<p>输出: 簇划分$C$</p>
<ol>
<li>
<p>初始化核心对象集合$\Omega = \varnothing$，初始化聚类簇数$k=0$，初始化未访问样本集合$\Gamma = D$，簇划分$C=\varnothing$</p>
</li>
<li>
<p>对于$j = 1,2,...,m$，按下面的步骤找出所有的核心对象：</p>
</li>
</ol>
<p>(a) 通过距离度量方式，找到样本$x_j$的$\varepsilon-$邻域子样本集$N_\varepsilon(x_j)$</p>
<p>(b) 如果子样本集中样本个数满足$|N_\varepsilon| \geq MinPts$，将样本$x_j$加入核心对象样本集合: $\Omega = \Omega \bigcup {x_j}$</p>
<ol>
<li>
<p>如果核心对象集合$\Omega = \varnothing$，则算法结束，否则转入步骤4</p>
</li>
<li>
<p>在核心对象集合$\Omega$中，随机选择一个核心对象$o$，初始化当前簇核心对象队列$\Omega_{cur}={0}$，初始化类别序号$k=k+1$，初始化当前簇样本集合$C_k = {o}$，更新未访问样本集合$\Gamma = \Gamma - {o}$</p>
</li>
<li>
<p>如果当前簇核心对象队列$\Omega_{cur} = \varnothing$，则当前聚类簇$C_k$生成完毕, 更新簇划分$C={C_1,C_2,...,C_k}$，更新核心对象集合$\Omega = \Omega - C_k$，转入步骤3</p>
</li>
<li>
<p>在当前簇核心对象队列$\Omega_{cur}$中取出一个核心对象$o'$，通过邻域距离阈值$\varepsilon$找出所有的$\varepsilon-$邻域子样本集$N_\varepsilon(o')$，令$\bigtriangleup = N_\varepsilon(o') \bigcap \Gamma$，更新当前簇样本集合$C_k = C_k \bigcup \bigtriangleup$，更新未访问样本集合$\Gamma = \Gamma - \bigtriangleup$，更新$\Omega_{cur} = \Omega_{cur} \bigcup (N_\varepsilon(o') \bigcap \Omega)$，转入步骤5</p>
</li>
</ol>
<p>输出结果为: 簇划分$C = {C_1,C_2,...,C_k}$</p>
<p>和传统的K-Means算法相比，DBSCAN最大的不同就是不需要输入类别数$k$，当然它最大的优势是可以发现任意形状的聚类簇，而不是像k-means，仅适用于凸样本集聚类。另外，它在聚类的同时还可以找出异常点，这点和BIRCH算法类似。</p>
<p>那么我们什么时候需要用DBSCAN来聚类呢？一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多。如果数据集不是稠密的，则不推荐用DBSCAN来聚类。</p>
<p>下面对DBSCAN算法的优缺点做一个总结。</p>
<p>DBSCAN的优点:
- 可以对分布特殊(非凸，互相包络，长条形等)的稠密数据集进行聚类
- 可以在聚类的同时发现异常点或噪声，从而对异常点或噪声不敏感 
- 不需要事先指定簇类的个数 </p>
<p>缺点:
- 当样本集的密度分布不均匀时，聚类质量较差
- 如果样本集较大时，聚类收敛时间较长，此时在搜索最近邻时需要建立KD树或者球树来进行算法加速。
- 需要对距离阈值$\varepsilon$和邻域样本数阈值$MinPts$联合调参，不同的参数组合对最后的聚类效果有较大的影响。
- 对于高维数据距离的计算会比较麻烦，容易造成“维数灾难”</p>
<h3 id="33">3.3 层次聚类<a class="headerlink" href="#33" title="Permanent link">#</a></h3>
<p>层次聚类(bierarchical clustering)试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用"自底向上"的聚合策略，也可采用"自顶层向下"的分拆策略。</p>
<p>AGNES是一种采用自底向上聚合策略的层次聚类算法，它先将数据集中的每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直至达到预设的聚类簇个数。这里的关键是如何计算聚类簇之间的距离。实际上，每个簇是一个样本集合，因此，只需采用关于集合的某种距离即可。例如，给定聚类簇$C_i$与$C_j$，可通过下面的式子来计算距离:</p>
<ul>
<li>最小距离: $d_{min}(C_i,C_j) = min_{x \in C_i,z\ in C_j}dist(x,z)$</li>
<li>最大距离: $d_{max}(C_i,C_j) = max_{x \in C_i,z \in C_j}dist(x,z)$</li>
<li>平均距离: $d_{avg}(C_i,C_j) = \dfrac{1}{|C_i||C_j|}\sum \limits_{x \in C_i} \sum \limits_{z \in C_j} dist(x,z)$</li>
</ul>
<p>显然，最小距离由两个簇的最近样本决定，最大距离由两个簇的最远样本决定，而平均距离则由两个簇的所有样本共同决定，当聚类簇距离由$d_{min}$ 、$d_{max}$或$d_{avg}$计算时，AGNES算法被相应地称为"单链接"(single-linkage) 、"全链接"(complete-linkage)或"均链接"(average-linkage)算法。</p>
<p>层次聚类算法如下:</p>
<p>输入: 样本集$D={x_1,x_2,...,x_m}$; 聚类簇距离度量函数$d$; 聚类簇数$k$</p>
<p>过程:
1:     for $j=1,2,...,m$ do
2:        $C_j={x_j}$
3:     end for
4:     for $i=1,2,...,m$ do
5:        for $j=1,2,...,m$ do
6:           $M(i,j)=d(C_i,C_j)$;
7:           $M(j,i)=M(i,j)
8:         end for
9:    end for
10:  设置当前聚类簇个数: $q=m$
11:   while $q&gt;k$ do
12:       找出距离最近的两个聚类簇$C_{i^<em>}$和$C_{j^</em>}$;
13:   合并$C_{i^<em>}$和$C_{j^</em>}$: $C_{i^<em>}=C_{i^</em>} \bigcup C_{j^<em>}$;
14:   for $j=j^</em>+1,j^<em>+2,...,q$ do
15:       将聚类簇$C_j$重新编号为$C_{j-1}$
16:   end for
17:   删除距离矩阵$M$的第$j^</em>$行与第$j^<em>$列;
18:   for $j=j^</em>+1,j^<em>+2,...,q-1$ do
19:       $M(i^</em>,j)=d(C_{i^<em>},C_j)$;
20:       $M(j,i^</em>)=M(i^*,j)
21:   end for
22:   $q=q-1$
23:   end while
输出: 簇划分$C={C_1,C_2,...,C_k}$</p>
<p>层次聚类算法的优点:
- 可以通过聚类树了解整个聚类过程
- 想要分多少个簇类都可以直接根据树结构来得到，改变簇类数目不需要再次计算数据点的归属</p>
<p>缺点:
- 计算量较大，每次都要计算多个簇类内所有数据点的两两距离。</p>
<h2 id="4">4 聚类性能评估<a class="headerlink" href="#4" title="Permanent link">#</a></h2>
<p>聚类算法作为非监督学习的一员，由于不存在数据标注，因此其性能的评价不像评估监督学习算法那样简单直接，需要许多指标来进行评估。</p>
<h3 id="41_adjusted_rand_index">4.1 调整兰德指数(Adjusted Rand Index)<a class="headerlink" href="#41_adjusted_rand_index" title="Permanent link">#</a></h3>
<p>想要理解调整兰德指数，就必须先说一下兰德指数。</p>
<p>假定样本集合为$S={o_1,o_2,...,o_n}$，给出聚类类别结果表示为$X={X_1,X_2,...,X_R}$，参考类别结果为$Y={Y_1,Y_2,...,Y_K}$，下面给出一些概念: </p>
<ul>
<li>集合$S_a = { (x_i, x_j) | x_i, x_j \in X_r； x_i, x_j \in Y_k }$，表示无论是聚类类别结果和参考类别结果，集合中的样本配对$x_i,x_j$都被分到了同一类别。a代表集合$S_a$中样本配对的数量。</li>
<li>集合$S_b = { (x_i, x_j) | x_i \in X_{r1}, x_j \in X_{r2}； x_i \in Y_{k1}, x_j \in Y_{k2} }$，表示无论是聚类类别结果和参考类别结果，集合中的样本配对$x_i,x_j$都没有被分到了同一类别。b代表集合$S_b$中样本配对的数量。</li>
<li>集合$S_c = { (x_i, x_j) | x_i, x_j \in X_r； x_i \in Y_{k1}, x_j \in Y_{k2} }$，表示在聚类类别结果中，集合$c$中的样本配对$x_i,x_j$属于同一类别； 在参考类别结果中，集合中的样本配对$x_i,x_j$不属于同一类别。c代表集合$S_c$中样本配对的数量。</li>
<li>集合$S_d = { (x_i, x_j) | x_i \in X_{r1}, x_j \in X_{r2}； x_i, x_j \in Y_k }$，表示在聚类类别结果中，集合中的样本配对$x_i,x_j$不属于同一类别； 在参考类别结果中，集合中的样本配对$x_i,x_j$属于同一类别。d代表集合$S_d$中样本配对的数量。</li>
</ul>
<p>那么兰德指数可以定义为
<script type="math/tex; mode=display">
RI = \dfrac{a+b}{a+b+c+d} = \dfrac{a+b}{\left(\begin{matrix} n \\ 2 \end{matrix}\right)}
</script>
$a+b$是聚类类别结果和参考类别结果一致的样本配对数量，相应的，$c+d$是不一致的样本配对数量，两者之和就是所有样本配对的总和，即从$n$个样本中随意抽取2个的组合数，也就是$\left(<script type="math/tex; mode=display">\begin{matrix} n \\ 2 \end{matrix}</script>\right) = \dfrac{n(n-1)}{2}$。</p>
<p>观察兰德指数的数学定义，不难发现兰德指数是个在0~1之间的数，等于0表示聚类结果和参考结果两者完全不一致，等于1表示聚类结果和参考结果两者完全一致。</p>
<p>在聚类结果随机产生的情况下，兰德指数不能保证为零，于是调整兰德系数(Adjusted Rand Index)被提出，它具有更高的区分度，它的数学表达式为: 
<script type="math/tex; mode=display">
ARI = \dfrac{RI-E(RI)}{max(RI)-E(RI)}
</script>
ARI的取值范围为$[-1,1]$，值越大意味着聚类结果与真实情况越吻合。从广义的角度来讲，ARI衡量的是两个数据分布的吻合程度。</p>
<p>优点:</p>
<ul>
<li>
<p>对任意数量的聚类中心和样本数，随机聚类的ARI都非常接近于0；</p>
</li>
<li>
<p>取值在$[-1,1]$之间，负数代表结果不好，越接近于1越好；</p>
</li>
<li>
<p>对聚类结构没有先验假设要求。</p>
</li>
</ul>
<p>缺点:</p>
<ul>
<li>需要真实的数据标签。</li>
</ul>
<h3 id="42">4.2 基于互信息的评分<a class="headerlink" href="#42" title="Permanent link">#</a></h3>
<p>互信息(Mutual Information)用来衡量两个数据分布的吻合程度。假设$U$与$V$是对$N$个样本标签的分配情况，则两种分布的熵(熵表示的是不确定程度)分别为：
<script type="math/tex; mode=display">
H(U)=\sum \limits_{i=1}^{|U|}P(i)log(P(i))
H(Ｖ)=\sum \limits_{ｊ=1}^{|Ｖ|}P'(j)log(P'(j))
</script>
其中，$P(i)=\frac{U_i}{N}$，$P'(j)=\frac{V_j}{N}$。</p>
<p>U与V之间的互信息(MI)定义为：
<script type="math/tex; mode=display">
MI(U,V)=\sum \limits_{i=1}^{|U|} \sum \limits_{j=1}^{|V|}P(i,j)log(\frac{P(i,j)}{P(i)P'(j)})
</script>
其中：$P(i,j)=\frac{|U_i \bigcap V_j|}{N}$</p>
<p>标准化后的互信息为：
<script type="math/tex; mode=display">
NMI(U,V)=\frac{MI(U,V)}{\sqrt{H(U)H(V)}}
</script>
不管标签分配之间的“互信息”的实际数量如何，互信息或者标准化后的值不会因此而调整，而会随着标签（簇）数量的增加而增加。</p>
<p>与ARI类似，调整互信息(Adjusted Mutual Information)定义为：
<script type="math/tex; mode=display">
AMI=\frac{MI-E[MI]}{max(H(U),H(V))-E[MI]}
</script>
利用基于互信息的方法来衡量聚类效果需要实际类别信息，MI与NMI取值范围为[0,1]，AMI取值范围为[-1,1]，它们都是值越大意味着聚类结果与真实情况越吻合。</p>
<p>优点:
- 对任意数量的聚类中心和样本数，随机聚类的AＭI都非常接近于0；</p>
<ul>
<li>取值在$[-1,1]$之间，负数代表结果不好，越接近于1越好；</li>
</ul>
<p>缺点:</p>
<ul>
<li>对数据结构有先验假设要求。</li>
</ul>
<h3 id="43_v-">4.3 同质性，完整性和V-度量<a class="headerlink" href="#43_v-" title="Permanent link">#</a></h3>
<p>如果给定样本标签的信息，有可能使用条件熵分析来定义一些直观的度量 。
- 同质性(homogeneity): 每个类只包含一个类的成员
- 完整性(completeness): 给定类别的所有成员最后都分配到该类</p>
<p>V-度量实际上等同于之前的归一化互信息(NMI)。</p>
<p>优点:</p>
<ul>
<li>分数是有界的，0代表效果差，1代表效果好</li>
<li>具有V-度量数值很差，可以通过同质性和完整性方面来进行定性分析</li>
<li>对聚类结构无任何先验假设要求</li>
</ul>
<p>缺点:
- 随机标记不会接近零，特别是当样本数量大时
- 当样本总数大于1000而样本类别不足10个时，以上指标效果不好</p>
<p>同质性(homogeneity)和完整性(completeness)的数学表达式可以由下面公式给出:
<script type="math/tex; mode=display">
h=1-\dfrac{H(C|K)}{H(C)}
</script>
</p>
<p>
<script type="math/tex; mode=display">
c=1-\dfrac{H(K|C)}{H(K)}
</script>
其中$H(C|K)$是给定样本标签的类的条件熵，由下式给出:
<script type="math/tex; mode=display">
H(C|K)=-\sum \limits_{c=1}^{|C|} \sum \limits_{k=1}^{|K|} \dfrac{n_{c,k}}{n} \log(\dfrac{n_{c,k}}{n})
</script>
并且$H(C)$是类别的熵，由下式给出:
<script type="math/tex; mode=display">
H(C)=-\sum \limits_{c=1}^{|C|} \dfrac{n_c}{n} log(\dfrac{n_c}{n})
</script>
其中$n$是样本总数，$n_c$和$n_k$分别属于$c$类和$k$类的样本数量，最后$n_{c,k}$是本该属于$c$类却被配给$k$类的样本的数量。</p>
<p>$H(K|C)$和$H(C)$的定义与上面同理。</p>
<p>进一步可以由同质性和完整性定义V-measure，其表达式为:
<script type="math/tex; mode=display">
v=2 \dfrac{h \cdot c}{h+c}
</script>
</p>
<h3 id="44_fowlkes-mallows">4.4 Fowlkes-Mallows分数<a class="headerlink" href="#44_fowlkes-mallows" title="Permanent link">#</a></h3>
<p>样本的真实标签已知才可以使用 Fowlkes-Mallows指数。Fowlkes-Mallows指数FMI被定义为:
<script type="math/tex; mode=display">
FMI = \dfrac{TP}{\sqrt{(TP+FP)(TP+FN)}}
</script>
其中的TP是True Positive(真正例)的数量，FP是False Positive(假正例)，FN是False Negative(假负例)的数量。</p>
<p>指数范围是[0,1]。越接近于1表示两个簇类之间越相似。</p>
<p>优点:</p>
<ul>
<li>
<p>对任意数量的聚类数目和样本数目，随机聚类的FMI都非常接近于0；</p>
</li>
<li>
<p>取值在$[0,1]$之间，零代表聚类结果完全不正确，越接近于1就表示聚类越合理；</p>
</li>
<li>
<p>对聚类结构没有先验假设要求。</p>
</li>
</ul>
<p>缺点:</p>
<ul>
<li>需要真实的数据标签。</li>
</ul>
<h3 id="45_silhouette">4.5 Silhouette系数<a class="headerlink" href="#45_silhouette" title="Permanent link">#</a></h3>
<p>如果样本的真实标签未知，那只能从模型本身来进行度量，Silhouette系数就是这样的一个评估例子。Silhouette系数的数学表达式由两项组成:</p>
<ul>
<li>
<p>a: 样本与同一类别中所有其他点的平均距离。</p>
</li>
<li>
<p>b: 样本与下一个距离最近的类别中所有其他点的平均距离。</p>
</li>
</ul>
<p>然后给出单个样本的Silhouette系数:
<script type="math/tex; mode=display">
s = \dfrac{b-a}{max(a,b)}
</script>
对整个数据集的Silhouette系数是单个数据的Silhouette系数的平均值。</p>
<p>优点:</p>
<ul>
<li>Silhouette系数的范围为[-1,1]，-1代表该数据没有被正确地聚类，+1代表该数据被正确地聚类，零点附近表示该数据点是两个类别之间有所重叠的数据点。</li>
<li>当类内数据点比较稠密而类间分隔比较开时，Silhouette系数得分较高。</li>
</ul>
<p>缺点:</p>
<ul>
<li>凸簇类集的Calinski-Harabaz指数通常高于其他类型的簇类，也就是Calinski-Harabaz指数比较受限于凸簇类集。</li>
</ul>
<h2 id="46_calinski-harabaz">4.6 Calinski-Harabaz指数<a class="headerlink" href="#46_calinski-harabaz" title="Permanent link">#</a></h2>
<p>对于$k$个类，Calinski-Harabaz指数的数学计算公式为：
<script type="math/tex; mode=display">
s(k) = \dfrac{Tr(B_k)}{Tr(W_k)} \dfrac{N-k}{k-1}
</script>
其中，$B_k$是类间距离矩阵，$Ｗ_k$是类内距离矩阵。它们的定义为：
<script type="math/tex; mode=display">
W_k=\sum \limits_{q=1}{k} \sum \limits_{x \in C_q}(x-c_q)(x-c_q)^T
</script>
</p>
<p>
<script type="math/tex; mode=display">
B_k=\sum_{q}n_q(c_q-c)(c_q-c)^T
</script>
$N$为数据总量，$C_q$是第$q$类的集合，$c_q$是第$q$类集合的中心点，$c$是所有数据点的中心，$n_q$是类别$q$所包含的数据数量。
类别内部数据的协方差越小越好，类别之间的协方差越大越好，这样的Calinski-Harabasz指数会更大。</p>
<p>优点:
- 当类内数据点比较稠密而类间分隔比较开时，Calinski-Harabasz指数得分较高。
- Calinski-Harabasz指数的计算速度快。</p>
<p>缺点:
- 凸簇类集的Calinski-Harabaz指数通常高于其他类型的簇类，也就是Calinski-Harabaz指数比较受限于凸簇类集。</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../202-ml-basics-07/" class="btn btn-neutral float-right" title="集成学习">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../202-ml-basics-05/" class="btn btn-neutral" title="PCA"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../202-ml-basics-05/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../202-ml-basics-07/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../mathjaxhelper.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
