<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>优化算法 - ai2.1 课程大纲与知识点</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "\u4f18\u5316\u7b97\u6cd5";
    var mkdocs_page_input_path = "202-ml-basics/202-ml-basics-08.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> ai2.1 课程大纲与知识点</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../..">前言</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">机器学习基础</span>
    <ul class="subnav">
                <li class="">
                    
    <span class="caption-text">机器学习入门</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../201-ml-intro/">机器学习入门</a>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">基本概念</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-01/">机器学习简介</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-02/">线性回归</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-03/">logitstic回归</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-04/">损失</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-05/">梯度下降</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-basics-06/">正则化/weight decay</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">应用技巧</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-skills-07/">非均衡数据</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-skills-08/">Hard-Example</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../../201-ml-basics/201-ml-intro-skills-09/">Early Stop</a>
                </li>
                <li class=" current">
                    
    <span class="caption-text">机器学习基础算法</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-01/">支持向量机</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-02/">决策树</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-03/">随机森林</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-04/">GBDT</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-05/">PCA</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-06/">聚类算法</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../404.md">吸引力传播</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../202-ml-basics-07/">集成学习</a>
                </li>
                <li class="toctree-l3 current">
                    
    <a class="current" href="./">优化算法</a>
    <ul class="subnav">
            
    <li class="toctree-l4"><a href="#_1">优化算法</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#_2">梯度下降法</a></li>
        
            <li><a class="toctree-l5" href="#_6">牛顿法</a></li>
        
            <li><a class="toctree-l5" href="#_7">拟牛顿法</a></li>
        
            <li><a class="toctree-l5" href="#_8">共轭梯度法</a></li>
        
            <li><a class="toctree-l5" href="#_9">坐标下降</a></li>
        
        </ul>
    

    </ul>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">机器学习算法进阶</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../203-ml-advance/203-ml-advance-03/">异常检测</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../404.md">最大期望算法</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">深度学习入门</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/">深度学习入门</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-01/">深度学习基础</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-02/">深度学习历史发展</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-03/">感知器与优化规则</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-04/">异或问题</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-05/">神经网络简介</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../211-dl-intro/211-dl-intro-06/">全连接网络</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">深度学习基础算法</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../212-dl-basics/">深度学习基础算法</a>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">基础算法</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-01/">神经网络前向传播</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-02/">神经网络拟合数据</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-03/">神经网络反向传播</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">调参</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-04/">过拟合与欠拟合</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-05/">学习率/lr decay</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-06/">优化算法：动量</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-07/">正则化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-08/">进一步讨论损失函数</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-09/">进一步讨论激活函数</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-10/">参数的初始化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-11/">batch norm</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-12/">dropout</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-13/">蒸馏法/transfer learning</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-14/">深度置信网络</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../212-dl-basics/212-dl-basics-15/">自编码器</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">视觉</span>
    <ul class="subnav">
                <li class="">
                    
    <span class="caption-text">计算机视觉基础</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <span class="caption-text">视觉介绍</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-01/">动物视觉介绍</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-02/">计算机视觉介绍</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-03/">视觉系统</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-04/">视觉认知</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">数字图像基础</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-05/">图像信号的数学表示</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-06/">图像的采样和量化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-07/">像素连通性</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-08/">rgb/bgr/lab/yuv/hsv/cmy及换算公式</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-09/">视频压缩与图像显示</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../301-cv-basics/301-cv-basics-10/">图像的线性系统理论</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">常用CV算法</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <span class="caption-text">图像处理与变换</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-01/">二维傅立叶变换及其基本性质</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-02/">快速傅立叶变换</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-03/">离散小波变换</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-04/">象素间的连通性</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-05/">灰度/彩色直方图</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-06/">图像空域滤波技术</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-07/">图像频域滤波</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-08/">多光谱图像处理</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-09/">颜色特征的图像检索</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-10/">图像金字塔</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-11/">二值化/大津算法/开闭操作</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-12/">边缘检测：sobel/laplance/canny</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-13/">泛洪填充</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">视觉几何基础</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-14/">坐标系与坐标变化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-15/">多视角</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-16/">标定</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-17/">双目与景深</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-18/">哈夫变换</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-19/">姿态与空间重建/多图使用</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">视觉认知模型</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-20/">微分算子</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-21/">阈值分割</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-22/">区域生长</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-23/">评价测度</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-24/">图搜索</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-25/">动态规划</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-26/">灰度共生矩阵</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-27/">基于模型的纹理分析</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-28/">运动估计简介/跟踪</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-29/">光流法</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">识别与高层认知</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-30/">图像的特征点</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-31/">Harris算法</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-32/">SIFT/SURF算法</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-33/">HOG算子检测</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-34/">距离</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-35/">统计分类方法</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-36/">马尔科夫随机场</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-37/">条件随机场</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-38/">模板匹配</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-39/">目标匹配</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-40/">特征内容匹配</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-41/">Marr视觉计算理论</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">应用</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-42/">多序列的三维重建</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../302-cv-algorithms/302-cv-algorithms-43/">数字图书馆藏查询</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">OpenCV</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../303-cv-opencv/303-cv-opencv-01/">opencv简介</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../303-cv-opencv/303-cv-opencv-02/">openCV安装</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../303-cv-opencv/303-cv-opencv-03/">图片存取与显示</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../303-cv-opencv/303-cv-opencv-04/">颜色转换</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../303-cv-opencv/303-cv-opencv-05/">各经典算法的api及文档导读</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">计算机视觉与神经网络</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <span class="caption-text">卷积神经网络</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-01/">卷积计算</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-02/">初始化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-03/">传统卷积使用</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-04/">卷积神经网络介绍</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-05/">池化</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-06/">特征的使用</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-07/">业务网络设计</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-08/">分类器的概念</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">卷积神经网络案例</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-09/">VGG</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-10/">GoogleLeNet</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-11/">resnet</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-12/">densenet</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-13/">nasnet</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-14/">se-net</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-15/">MobileNet v1/v2</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-16/">已被证明有效的基础模块</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l3">
                    
    <span class="caption-text">卷积神经网络应用</span>
    <ul class="subnav">
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-17/">分类</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-18/">检测</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-19/">分割</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-20/">人脸</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-21/">各种娱乐项目（style transher/deepdream/nima/gan）</a>
                </li>
                <li class="toctree-l4">
                    
    <a class="" href="../../311-cv-nn/311-cv-nn-22/">backbone</a>
                </li>
    </ul>
                </li>
    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../321-nlp-intro/">自然语言处理NLP</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">附录</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../notation/">符号约定</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">ai2.1 课程大纲与知识点</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>机器学习基础算法 &raquo;</li>
        
      
        
          <li>机器学习基础 &raquo;</li>
        
      
    
    <li>优化算法</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="_1">优化算法<a class="headerlink" href="#_1" title="Permanent link">#</a></h1>
<h2 id="_2">梯度下降法<a class="headerlink" href="#_2" title="Permanent link">#</a></h2>
<h3 id="_3">梯度下降法算法详解<a class="headerlink" href="#_3" title="Permanent link">#</a></h3>
<h4 id="1">1. 梯度下降的直观解释<a class="headerlink" href="#1" title="Permanent link">#</a></h4>
<p>首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。</p>
<p>从上面的解释可以看出，梯度下降不一定能够找到全局最优解，有可能找到的是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<p><img alt="202-ml-basics-08" src="202-ml-basics-08-01-youhua.png" /></p>
<h4 id="2">2. 梯度下降的相关概念<a class="headerlink" href="#2" title="Permanent link">#</a></h4>
<p>在详细了解梯度下降的算法之前，我们先看看相关的一些概念。</p>
<ol>
<li>
<p>步长(Learning rate)：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。</p>
</li>
<li>
<p>特征(feature)：指的是样本中输入部分，比如单特征的2个样本$(x(0),y(0)),(x(1),y(1))$，第一个样本特征为$x(0)$，第一个样本输出为$y(0)$。</p>
</li>
<li>
<p>假设函数(hypothesis function)：在监督学习中，为了拟合输入样本而使用的假设函数，记为$h_\theta(x)$。比如对于单特征的$m$个样本$(x(i),y(i))(i=1,2,...m)$，可以采用拟合函数如下：$h_\theta(x)=\theta_0+θ_1x$。</p>
</li>
<li>
<p>损失函数(loss function)：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于$m$个样本$(x_i,y_i)(i=1,2,...m)$，采用线性回归，损失函数为：
   <script type="math/tex; mode=display">
   J(\theta_0,\theta_1)=\sum_{i=1}^m(h_\theta(x_i)−y_i)^2
   </script>
   其中$x_i$表示第$i$个样本特征，$y_i$表示第$i$个样本对应的输出，$h_\theta(x_i)$为假设函数。</p>
</li>
</ol>
<h4 id="3">3. 梯度下降的详细算法<a class="headerlink" href="#3" title="Permanent link">#</a></h4>
<h5 id="31">3.1 梯度下降法的代数方式描述<a class="headerlink" href="#31" title="Permanent link">#</a></h5>
<p>梯度下降法算法有代数法和矩阵法(也称向量法)两种表示，代数法比较容易理解，而矩阵法会显得更加简洁，由矩阵实现的逻辑更加一目了然。这里先介绍代数法，后介绍矩阵法。</p>
<ol>
<li>
<p>先决条件： 确认优化模型的假设函数和损失函数。
   比如对于线性回归，假设函数表示为$h_\theta(x_1,x_2,...x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n$，其中$\theta_i(i = 0,1,2,..., n)$为模型参数，$x_i (i = 0,1,2... n)$为每个样本的$n$个特征值。这个表示可以简化，我们增加一个特征$x_0=1$，这样$h_\theta(x_0,x_1,...,x_n)=\sum_{i=0}^nθ_ix_i$。
   同样是线性回归，对应于上面的假设函数，损失函数为：
   <script type="math/tex; mode=display">
   J(θ_0,θ_1,...,θ_n)=\frac{1}{m}\sum_{j=1}^m(h_\theta(x_0(j),x_1(j),...x_n(j))−y_j)^2
   </script>
</p>
</li>
<li>
<p>算法相关参数初始化：主要是初始化$\theta_0,\theta_1,...,\theta_n$，算法终止距离$\varepsilon$以及步长$\alpha$。在没有任何先验知识的时候，我喜欢将所有的$\theta$初始化为0， 将步长初始化为1。在调优的时候再优化。</p>
</li>
<li>
<p>算法过程：
   1)确定当前位置的损失函数的梯度，对于$\theta_i$，其梯度表达式如下：
   <script type="math/tex; mode=display">
   \frac{\partial J(θ_0,θ_1...,θ_n)}{\partial \theta_i}
   </script>
   2)用步长乘以损失函数的梯度，得到当前位置下降的距离，即$\alpha\frac{\partial J(θ_0,θ_1...,θ_n)}{\partial θ_i}$。对应前面登山例子中的某一步。</p>
</li>
</ol>
<p>3)确定是否所有$\theta_i$的梯度下降距离都小于$\varepsilon$，如果小于$\varepsilon$则算法终止，当前所有的$\theta_i(i=0,1,...n)$即为最终参数结果。否则进入步骤4.</p>
<p>4)更新所有的$\theta$，对于$\theta_i$，其更新表达式如下。更新完毕后继续转入步骤1.
   <script type="math/tex; mode=display">
   θ_i=θ_i−\alpha\frac{\partial J(\theta_0,\theta_1,...,\theta_n)}{ \partial \theta_i}
   </script>
</p>
<p>下面用线性回归的例子来具体描述梯度下降。假设我们的样本是
<script type="math/tex; mode=display">
(x_1(0),x_2(0),...,x_n(0),y_0),(x_1(1),x_2(1),...,x_n(1),y_1),...,(x_1(m),x_2(m),...x_n(m),y_m)
</script>
损失函数如前面先决条件所述：
<script type="math/tex; mode=display">
J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{m}\sum_{j=1}^{m}(h_\theta(x_0(j),x_1(j),...x_n(j))−y_j)^2
</script>
算法过程步骤1中对于$θ_i$的偏导数计算如下： 　　
<script type="math/tex; mode=display">
\frac {\partial J(\theta_0,\theta_1,...,\theta_n)}{\partial \theta_i}=\frac{2}{m}\sum_{j=1}^m(h_\theta(x_0(j),x_1(j),...,x_n(j))−y_j)\frac{\partial{h_\theta}}{\partial \theta_i}
</script>
由于样本中没有$x_0$，令上式中所有的$x_0(j)$为1。步骤4中$\theta_i$的更新表达式如下：
<script type="math/tex; mode=display">
\theta_i=\theta_i−\frac{2\alpha}{m}\sum_{j=1}^m(h_\theta(x_0(j),x_1(j),...,x_n(j))−y_j)\frac{\partial{h_\theta}}{\partial \theta_i}
</script>
从这个例子可以看出当前点的梯度方向是由所有的样本决定的，加$\frac{2}{m}$是为了好理解。由于步长为常数，他们的乘积也为常数，所以这里$\frac{2\alpha}{m}$可以用一个常数$\alpha$表示。</p>
<p>在下面第4节会详细讲到梯度下降法的变种，他们主要的区别就是对样本的采用方法不同。这里我们采用的是所有样本。</p>
<h5 id="32">3.2 梯度下降法的矩阵方式描述<a class="headerlink" href="#32" title="Permanent link">#</a></h5>
<p>这一部分主要讲解梯度下降法的矩阵表示方法，相对于2.3.3.1的代数法，要求有一定的矩阵分析基础知识，尤其是矩阵求导的知识。</p>
<ol>
<li>先决条件： 和2.3.3.1类似， 需要确认优化模型的假设函数和损失函数。对于线性回归，假设函数为
   <script type="math/tex; mode=display">
   h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n
   </script>
   分别代入特征值$x_i(1),x_i(2),...,x_i(m)$，$i=0,1,...,n$，得到
   <script type="math/tex; mode=display">
   h_\theta(x_1(1),x_2(1),...,x_n(1))=\theta_0+\theta_1x_1(1)+...+\theta_nx_n(1)
   </script>
</li>
</ol>
<p>
<script type="math/tex; mode=display">
   h_\theta(x_1(2),x_2(2),...,x_n(2))=\theta_0+\theta_1x_1(2)+...+\theta_nx_n(2)
   </script>
</p>
<p>
<script type="math/tex; mode=display">
   ...
   </script>
</p>
<p>
<script type="math/tex; mode=display">
   h_\theta(x_1(m),x_2(m),...,x_n(m))=\theta_0+\theta_1x_1(m)+...+\theta_nx_n(m)
   </script>
</p>
<p>将上面的式子由矩阵来表达：
   <script type="math/tex; mode=display">
   h_\theta(X)=X\theta
   </script>
   其中，$X$为$m\times(n+1)$维的矩阵，$\theta$为$(n+1)\times1$的向量，那么假设函数$h_\theta(X)$就是$m\times1$的向量。</p>
<p>损失函数的矩阵表达式为：
   <script type="math/tex; mode=display">
   J(θ)=\frac{1}{m}(X\theta−Y)^T(X\theta−Y)
   </script>
   其中$Y$是样本的输出向量，维度为$m\times1$。</p>
<ol>
<li>
<p>算法相关参数初始化: $\theta$向量可以初始化为默认值$\mathbf{0}$，或者调优后的值。算法终止距离$\varepsilon$，步长$\alpha$和2.3.3.1比没有变化。</p>
</li>
<li>
<p>算法过程：</p>
</li>
</ol>
<p>1)确定当前位置损失函数的梯度，对于$\theta$向量,其梯度表达式如下：
   <script type="math/tex; mode=display">
   \frac{\partial J(\theta)}{\partial \theta}
   </script>
   2)用步长乘以损失函数的梯度，得到当前位置下降的距离，即$\alpha\frac{\partial J(\theta)}{\partial \theta}$对应于前面登山例子中的某一步。</p>
<p>3)确定$\theta$向量里面每个值的梯度下降距离都小于$\varepsilon$，如果小于$\varepsilon$则算法终止，当前$\theta$向量即为最终参数结果。否则进入步骤4.</p>
<p>4)更新$\theta$向量，其更新表达式如下。更新完毕后继续转入步骤1
   <script type="math/tex; mode=display">
   \theta=\theta−\frac{\partial J(θ)}{\partial \theta}
   </script>
   还是用线性回归的例子来描述具体的算法过程。</p>
<p>损失函数对于$\theta$向量的偏导数计算如下：
   <script type="math/tex; mode=display">
   \frac{\partial J(θ)}{\partial \theta}=X^T(X\theta−Y)
   </script>
   步骤4中$\theta$向量的更新表达式如下：$\theta=\theta−\alpha X^T(X\theta−Y)$</p>
<p>对于2.3.3.1的代数法，可以看到矩阵法要简洁很多。这里面用到了矩阵求导链式法则，和两个矩阵求导的公式。</p>
<p>公式1：$\frac{\partial (XX^T)}{\partial X}=2X$</p>
<p>公式2：$\frac{\partial X\theta}{\partial \theta}=X^T$</p>
<p>如果需要熟悉矩阵求导建议参考张贤达的《矩阵分析与应用》一书。</p>
<h3 id="_4">梯度下降的算法调优<a class="headerlink" href="#_4" title="Permanent link">#</a></h3>
<p>在使用梯度下降时，需要进行调优。哪些地方需要调优呢？</p>
<ol>
<li>
<p>算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上步长取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较优值。</p>
</li>
<li>
<p>算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，最后选择损失函数最小化的初值。</p>
</li>
<li>
<p>归一化。由于样本的不同特征有不一样的取值范围，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对每个特征$x$，求出它的期望$\mu(x)$和标准差$\sigma(x)$，然后转化为：
   <script type="math/tex; mode=display">
   \frac{x−\mu(x)}{\sigma(x)}
   </script>
   这样特征的新期望为0，新方差为1，迭代次数可以大大较少。</p>
</li>
</ol>
<h3 id="bgdsgdmbgd">梯度下降法大家族(BGD，SGD，MBGD)<a class="headerlink" href="#bgdsgdmbgd" title="Permanent link">#</a></h3>
<hr />
<h4 id="1_batch_gradient_descent">1.  批量梯度下降法(Batch Gradient Descent)<a class="headerlink" href="#1_batch_gradient_descent" title="Permanent link">#</a></h4>
<p>批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新，这个方法对应于前面2.3.3.1的线性回归的梯度下降算法，也就是说2.3.3.1的梯度下降算法就是批量梯度下降法。　　
<script type="math/tex; mode=display">
\theta_i=\theta_i−\alpha\sum_{j=1}^m(h_\theta(x_0(j),x_1(j),...,x_n(j))−y_j)\frac{\partial{h_\theta}}{\partial \theta_i}
</script>
由于我们有$m$个样本，这里求梯度的时候就用了所有$m$个样本的梯度数据。</p>
<h4 id="2_stochastic_gradient_descent">2.  随机梯度下降法(Stochastic Gradient Descent)<a class="headerlink" href="#2_stochastic_gradient_descent" title="Permanent link">#</a></h4>
<p>随机梯度下降法，其实和批量梯度下降法原理类似，区别在于求梯度时没有用所有的$m$个样本的数据，而是仅选取一个样本$j$来求梯度。对应的更新公式是：
<script type="math/tex; mode=display">
\theta_i=\theta_i−\alpha(h_\theta(x_0(j),x_1(j),...,x_n(j))−y_j)\frac{\partial{h_\theta}}{\partial \theta_i}
</script>
随机梯度下降法，和2.4.1的批量梯度下降法是两个极端，一个采用所有数据来梯度下降，另一个只用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</p>
<p>那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这就是小批量梯度下降法。</p>
<h4 id="3_mini-batch_gradient_descent">3.  小批量梯度下降法(Mini-batch Gradient Descent)<a class="headerlink" href="#3_mini-batch_gradient_descent" title="Permanent link">#</a></h4>
<p>小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于$m$个样本，我们采用其中的$k$个样本来迭代，$1&lt;k&lt;m$。一般可以取$k=10$，当然根据样本的数量，可以调整这个$k$值。对应的更新公式是：
<script type="math/tex; mode=display">
\theta_i=\theta_i−\alpha\sum_{j=1}^k(h_\theta(x_0(j),x_1(j),...,x_n(j))−y_j)\frac{\partial{h_\theta}}{\partial \theta_i}
</script>
</p>
<h3 id="_5">梯度下降法和其他无约束优化算法的比较<a class="headerlink" href="#_5" title="Permanent link">#</a></h3>
<hr />
<p>除了梯度下降外，机器学习中的无约束优化算法还有前面提到的最小二乘法、牛顿法和拟牛顿法。</p>
<p>和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法就需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，这种情况使用迭代的梯度下降法就比较有优势。</p>
<p>梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的Hessen矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快，但是每次迭代的时间比梯度下降法长。</p>
<h2 id="_6">牛顿法<a class="headerlink" href="#_6" title="Permanent link">#</a></h2>
<p>​   除了前面说的梯度下降法，牛顿法也是机器学习中用的比较多的一种优化算法。牛顿法的基本思想是利用迭代点$ x_k$处的一阶导数(梯度)和二阶导数(Hessen矩阵)对目标函数进行二次函数近似，然后把二次模型的极小点作为新的迭代点，并不断重复这一过程，直至求得满足精度的近似极小值。牛顿法的速度相当快，而且能高度逼近最优值。</p>
<p>​   在介绍牛顿法之前先介绍一下泰勒公式。泰勒公式就是用多项式函数去逼近光滑函数，是将一个在$x=x_0$处具有n阶导数的函数$f(x)$利用关于$(x−x_0)$的n次多项式来逼近函数的方法。  若函数$f(x)$在包含$x_0$的某个闭区间$[a,b]$上具有n阶导数，且在开区间$(a,b)$上具有$(n+1)$阶导数，则对闭区间$[a,b]$上任意一点$x$，成立下式：
<script type="math/tex; mode=display">
f(x) = f(x_k)+(x-x_k)f'(x_k)+\frac{1}{2!}(x-x_k)^2f''(x_k)+o^n
</script>
​   牛顿法的最初提出是用来求解方程的根的。我们假设点$x_<em>$为函数$f(x)$的根，那么有$f(x_</em>)=0$。现在我们把函数$f(x)$在点$x_k$处一阶泰勒展开有：
<script type="math/tex; mode=display">
f(x) = f(x_k) + f'(x_k)(x-x_k)
</script>
​   那么假设点$x_{k+1}$为该方程的根，则有：
<script type="math/tex; mode=display">
f(x_{k+1}) = f(x_k) + f'(x_k)(x_{k+1} - x_k) = 0
</script>
​   那么就可以得到：
<script type="math/tex; mode=display">
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_{k})}
</script>
​   图示如下：</p>
<p><img alt="202-ml-basics-08" src="202-ml-basics-08-02-youhua.png" /></p>
<p>​   对于最优化问题，其极值点处有一个特性就是在极值点处函数的一阶导数为0。因此我们可以在一阶导数处利用牛顿法通过迭代的方式来求得最优解，即相当于求一阶导数对应函数的根。 在现有极小点估计值的附近对$f(x)$做二阶泰勒展开，进而找到极小点的下一个估计值。设$x_k$为当前的极小点估计值，则：<br />
<script type="math/tex; mode=display">
f(x)=f(x_k)+f'(x_k)(x-x_k)+\frac{1}{2}f''(x_k)(x-x_k)^2
</script>
​   对上式求导并令其为0，则为：
<script type="math/tex; mode=display">
f'(x_k)+f''(x_k)(x-x_k)=0
</script>
​   既得到牛顿法的更新公式：
<script type="math/tex; mode=display">
x=x_k-\frac{f'(x_k)}{f''(x_k)}
</script>
​    这样我们就得到了一个不断更新$x$迭代求得最优解的方法。这个也很好理解，假设我们上面的第一张图的曲线表示的是函数$f(x)$一阶导数的曲线，那么其二阶导数就是一阶导数对应函数在某点的斜率，也就是那条切线的斜率，那么该公式就和上面求根的公式本质是一样的。  我们这里讨论的都是在低维度的情形下，那么对于高维函数，其二阶导数就变为了一个海森矩阵，记为$H(x)=[\frac{\delta ^2f}{\delta x_i\delta x_j}]$，那么迭代公式就变为了 :
<script type="math/tex; mode=display">
x^{k+1} = x^k - H_k^{-1}f'_k
</script>
​   我们可以看到，当$H_k$为正定（$H_k^{-1}$ 也为正定）的时候，可以保证牛顿法的搜索方向是向下搜索的。</p>
<p>​   牛顿法求最优值的步骤如下：</p>
<ol>
<li>随机选取起始点$x_0$；  </li>
<li>计算目标函数$f(x)$在该点$x^k$的一阶导数和海森矩阵；  </li>
<li>依据迭代公式$x^{k+1} = x^k - H_k^{-1}f'<em k_1="k+1">k​$更新$x​$值  如果$E(f(x</em>) - f(x_k)) &lt; \epsilon​$则收敛返回，否则继续步骤2,3直至收敛 。</li>
</ol>
<p>​        我们可以看到，当我们的特征特别多的时候，求海森矩阵的逆的运算量是非常大且慢的，这对于在实际应用中是不可忍受的，因此我们想能否用一个矩阵来代替海森矩阵的逆呢，这就是拟牛顿法的基本思路。</p>
<h2 id="_7">拟牛顿法<a class="headerlink" href="#_7" title="Permanent link">#</a></h2>
<p>​   因为我们要选择一个矩阵来代替海森矩阵的逆，那么我们首先要研究一下海森矩阵需要具有什么样的特征才能保证牛顿法成功的应用。通过上面的描述我们知道：
<script type="math/tex; mode=display">
f'(x^{k+1}) = f'(x^k)+H_k(x^{k+1}-x^k) \Rightarrow\\H_K^{-1}(f'(x^{k+1})-f'(x_k)) =x^{k+1}-x^k
</script>
​   上式我们称之为拟牛顿条件。  因此，对于我们所选择的替代矩阵$G_k$，需要满足两个条件：</p>
<ol>
<li>拟牛顿条件，即$G_k(f′(x_k+1)−f′(x_k))=x_k+1−x_k$；</li>
<li>要保证$G_k$为正定矩阵，这是因为只有正定才能保证牛顿法的搜索方向是向下搜索的。</li>
</ol>
<p>​        假设$y_k=f′(x_k+1)−f′(x_k)，δ_k=x_k+1−x_k$，因为每次迭代我们都需要更新替代矩阵$G_k$，下面介绍一种常用的迭代算法DFP(Davidon-Fletcher-Powell) 。</p>
<h3 id="dfp">DFP算法<a class="headerlink" href="#dfp" title="Permanent link">#</a></h3>
<p>​   DFP算法中选择$G_{k+1}$方法是在每一步迭代中在矩阵$G_k$中加两项附加项构成$G_{k+1}$，即：
<script type="math/tex; mode=display">
G_{k+1} = G_k +P_k + Q_k
</script>
​   这时：
<script type="math/tex; mode=display">
G_{k+1}y_k = G_ky_k +P_ky_k + Q_ky_k
</script>
​   为了使$G_{k+1}$满足拟牛顿条件，我们可以令$P_ky_k = \delta_k, Q_ky_k = -G_ky_k$，这样我们就得到$G_k$的迭代公式。</p>
<h2 id="_8">共轭梯度法<a class="headerlink" href="#_8" title="Permanent link">#</a></h2>
<p>​   共轭梯度方法也是一种迭代方法，理论上只要n步就能找到真解，实际计算中，考虑到舍入误差，一般迭代3n到5n步，每步的运算量相当与矩阵乘向量的运算量，对稀疏矩阵特别有效。共轭梯度法收敛的快慢依赖于系数矩阵的谱分布情况，当特征值比较集中，系数矩阵的条件数很小，共轭梯度方法收敛得就快。“超线性收敛性”告诉我们，实际当中，我们往往需要更少的步数就能得到所需的精度的解。</p>
<p>​   考虑线性对称正定方程组：
<script type="math/tex; mode=display">
Ax = b
</script>
​   空间中任意向量x都可以用一组相互共轭且线性无关的基向量表示：
<script type="math/tex; mode=display">
x=\sum_{i=1}^na_ip_i
</script>
</p>
<p>​   利用共轭性和矩阵的对称正定性我们定义：
<script type="math/tex; mode=display">
\alpha_i = \frac{\langle x,p_i\rangle}{\langle p_i,p_i\rangle}=\frac{x^TAp_i}{p_i^TAp_i}=\frac{b^Tp_i}{p_i^TAp_i},\quad \forall i=0,1,\cdots,n-1
</script>
    假设已经知道了 $p_0,p_1,⋯,p_{i−1}$, 我们记：
<script type="math/tex; mode=display">
p_ {i} = r_{i}-\sum_{k<i}\beta_k p_k
</script>
​   在上式两端对$p_k,k&lt;i$， 做内积有：
<script type="math/tex; mode=display">
\beta_k = \frac{\langle p_k,r_i\rangle}{\langle p_k,p_k\rangle}=\frac{p_k^TAr_i}{p_k^TAp_k}\quad \forall k<i
</script>
​   也就是说：
<script type="math/tex; mode=display">
p_ {i} = r_{i}-\sum_{k<i}\frac{p_k^TAr_i}{p_k^TAp_k} p_k
</script>
</p>
<p>​   接下来我们要选择$r_0,r_1,⋯,r_{n−1}$， 这里我们令：
<script type="math/tex; mode=display">
r_i = b-Ax_i,\quad x_i = \sum_{k<i}\alpha_kp_k,\quad i=0,1,\cdots,n-1\\r_{i+1} = b-Ax_{i+1}=b-A(x_i+\alpha_i p_i)=r_i-\alpha_iAp_i
</script>
​   以及：
<script type="math/tex; mode=display">
p_k^Tr_i = p_k^T(b-Ax_i)=p_k^Tb-\alpha_kp_k^TAp_k=0,\quad \forall k<i\\
p_k^TAr_i=\frac{1}{\alpha_k}(r_k-r_{k+1})^Tr_i
</script>
​   如果$k&lt;i$  :
<script type="math/tex; mode=display">
r_k^Tr_i=(p_k+\sum_{j<i}\beta_jp_j)^Tr_i=0
</script>
​   则：
<script type="math/tex; mode=display">
p_k^TAr_i=0,\quad \forall k<i-1,\quad p_{i-1}^TAr_i=-\frac{1}{\alpha_{i-1}}r_i^Tr_i
</script>
​   同样：
<script type="math/tex; mode=display">
p_k^TAp_k = p_k^TA(r_k-\sum_{j<k}\beta_jp_j)=p_k^TAr_k=\frac{1}{\alpha_k}r_k^Tr_k
</script>
​   则：
<script type="math/tex; mode=display">
p_i = r_i+\frac{r_i^Tr_i}{r_{i-1}^Tr_{i-1}}p_{i-1}
</script>
</p>
<p>​   化简后的$\alpha_i$：
<script type="math/tex; mode=display">
\alpha_i = \frac{b^Tp_i}{p_i^TAp_i} = \frac{(r_i+Ax_i)^Tp_i}{p_i^TAp_i}=\frac{r_i^Tp_i}{p_i^TAp_i}=\frac{r_i^Tr_i}{p_i^TAp_i}
</script>
</p>
<p>​   最后我们有如下算法：</p>
<ol>
<li>
<p>初始条件：$x_0=0,r_0=b,p_0=r_0$</p>
</li>
<li>
<p>迭代：
   <script type="math/tex; mode=display">
   \alpha_i = \frac{r_i^Tr_i}{p_i^TAp_i}\\x_{i+1}=x_i+\alpha_ip_i\\r_{i+1} = r_i-\alpha_iAp_i\\p_{i+1}=p_i+\frac{r_{i+1}^Tr_{i+1}}{r_{i}^Tr_{i}}p_{i}
   </script>
</p>
</li>
</ol>
<h2 id="_9">坐标下降<a class="headerlink" href="#_9" title="Permanent link">#</a></h2>
<p>​   坐标下降法属于一种非梯度优化的方法，它在每步迭代中沿一个坐标的方向进行搜索，通过循环使用不同的坐标方法来达到目标函数的局部极小值。 </p>
<p>​   算法过程：</p>
<p>​   For $i=1$  to $n$：  <br />
<script type="math/tex; mode=display">
x_i  := \mathop{\arg\min}_{\theta} F(x_1,x_2,…,x_{i-1},x_i,x_{i+1},…,x_n)
</script>
​   相当于每次迭代都只是更新$x$的一个维度，即把该维度当做变量，剩下的$n-1$个维度当作常量,通过最小化$f(x)$来找到该维度对应的新的值。坐标下降法就是通过迭代地构造序列$x_0,x_1,x_2,…$来求解问题，即最终点收敛到期望的局部极小值点。通过上述操作，显然有： 
<script type="math/tex; mode=display">
f(x^0)\geq f(x^1)\geq f(x^2)\geq\ldots
</script>
​   坐标下降法在每次迭代中在当前点处沿一个坐标方向进行一维搜索 ，固定其他的坐标方向，找到一个函数的局部极小值。坐标下降优化方法是一种非梯度优化算法。在整个过程中依次循环使用不同的坐标方向进行迭代，一个周期的一维搜索迭代过程相当于一个梯度迭代。gradient descent 方法是利用目标函数的导数（梯度）来确定搜索方向的，该梯度方向可能不与任何坐标轴平行。而coordinate descent方法是利用当前坐标方向进行搜索，不需要求目标函数的导数，只按照某一坐标方向进行搜索最小值。</p>
<p>​                 </p>
<p>​                    </p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../203-ml-advance/203-ml-advance-03/" class="btn btn-neutral float-right" title="异常检测">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../202-ml-basics-07/" class="btn btn-neutral" title="集成学习"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../202-ml-basics-07/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../203-ml-advance/203-ml-advance-03/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../mathjaxhelper.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
